{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MicroRaft MicroRaft is a feature-complete and stable open-source implementation of the Raft consensus algorithm in Java. It is released with the Apache 2 license . MicroRaft works on top of a minimalistic and modular design. It is a single lightweight JAR with a few hundred KBs of size and only logging dependency . It contains an isolated implementation of the Raft consensus algorithm, and a set of accompanying interfaces to run the algorithm in a multi-threaded and distributed environment. These interfaces surround the Raft consensus algorithm, and abstract away the concerns of persistence, thread-safety, serialization, networking and actual state machine logic. Developers are required to implement these interfaces to build CP distributed systems on top of MicroRaft. You can read the public announcement here. Features MicroRaft is a complete implementation of the Raft consensus algorithm. It implements the leader election, log replication, log compaction (snapshotting), and cluster membership changes components. Additionally, it realizes a rich set of optimizations and enhancements, as listed below, to allow developers to run Raft clusters in a reliable and performant manner, and tune its behaviour based on their needs. Adaptive batching during log replication Back pressure to prevent OOMEs on Raft leader and followers Parallel snapshot transfer from Raft leader and followers Pre-voting and leader stickiness ( \u00a7 4.2.3 and 9.6 of the Raft dissertation , and 4 Modifications for Raft Consensus ) Auto-demotion of Raft leader on loss of quorum heartbeats Linearizable quorum reads without appending log entries (\u00a7 6.4 of the Raft dissertation) Lease-based local queries on Raft leader (\u00a7 6.4.1 of the Raft dissertation) Monotonic local queries on Raft followers (\u00a7 6.4.1 of the Raft dissertation) Parallel disk writes on Raft leader and followers (\u00a7 10.2.1 of the Raft dissertation) Leadership transfer (\u00a7 3.10 of the Raft dissertation) Improved majority quorums Use cases MicroRaft can be used for building highly available and strongly consistent data, metadata and coordination services. An example of data service is a distributed key-value store. You can build a distributed key-value store where each partition / shard is maintained by a separate Raft cluster ( Raft group in MicroRaft terms). MicroRaft can be also used for building a control plane or coordination cluster. It can store the metadata of your large-scale data services. High-level APIs, such as leader election mechanisms, group membership management systems, distributed locks, distributed transaction managers, or distributed resource schedulers can be also built on top of MicroRaft. Please note that MicroRaft is not a high-level solution like a distributed key-value store, or a distributed lock service. It is a library that offers a set of abstractions and functionalities to help you build such high-level systems without intertwining your system with Raft code. Getting started Just run the following command on your terminal for a sneak peek at MicroRaft. It starts a 3-node local Raft group, elects a leader, and commits a number of operations. $ gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial If you want to learn more about how to use MicroRaft for building a CP distributed system, you can check out the Main Abstractions section first, and then read the tutorial to build an atomic register on top of MicroRaft. Getting involved MicroRaft is a new open-source library. Your contribution and feedback is welcome! The development happens on Github . You can follow @MicroRaft on Twitter for announcements. What is consensus? Consensus is one of the fundamental problems in distributed systems. It involves multiple servers agree on a value. Once a value is decided, the decision is final. Consensus algorithms are very useful in a plethora of distributed systems that require high availability and strong consistency. Paxos, first introduced by Leslie Lamport, is probably the most widely known consensus algorithm. However, it has been also known as difficult to reason about and lacking details for building practical implementations. Raft was introduced in 2013 as a new consensus algorithm with the main goal of understandability. Ever since its introduction, Raft has received widespread adoption in the industry. Raft approaches the consensus problem in the context of replicated state machines, where a group of servers applies the same set of operations and computes identical copies of the same state. Raft's primary enabler of understandability is the problem decomposition technique. It divides the consensus problem into 3 pieces: leader election, log replication and safety, and solves each piece relatively independently. Raft starts by electing a leader. There is a single functional leader managing the servers, and upon its failure a new leader is elected. Each server keeps a local log. Clients send their requests to the leader. The leader appends incoming requests into its log and replicates them to the other servers. Each server appends the requests sent by the leader into its log. Once a request is appended to the local logs of sufficient number (i.e., more than half) of servers, the leader considers the request committed, hence executes it on its local state machine, also notifies other servers to do the same. Raft orders requests by the indices they are appended to the replicated log. In addition, Raft's leader election and log replication rules ensure that once a request is committed and executed at a given log index on one server, no other server can execute another request for the same log index, including the presence of non-Byzantine failures. This is basically Raft's safety property. Thanks to this property, each server executes the same sequence of requests. Once these requests are deterministic, servers compute identical copies of the same state and produce the same output values. For more details about Raft, please see the In Search of an Understandable Consensus Algorithm paper by Diego Ongaro and John Ousterhout. Acknowledgements MicroRaft originates from Hazelcast IMDG's Raft implementation and includes several significant improvements on the public APIs and internals. MicroRaft's logo is created by modifying Raft's original logo . Raft's logo was created by Andrea Ruygt and licensed under the Creative Commons Attribution-4.0 International .","title":"Home"},{"location":"#microraft","text":"MicroRaft is a feature-complete and stable open-source implementation of the Raft consensus algorithm in Java. It is released with the Apache 2 license . MicroRaft works on top of a minimalistic and modular design. It is a single lightweight JAR with a few hundred KBs of size and only logging dependency . It contains an isolated implementation of the Raft consensus algorithm, and a set of accompanying interfaces to run the algorithm in a multi-threaded and distributed environment. These interfaces surround the Raft consensus algorithm, and abstract away the concerns of persistence, thread-safety, serialization, networking and actual state machine logic. Developers are required to implement these interfaces to build CP distributed systems on top of MicroRaft. You can read the public announcement here.","title":"MicroRaft"},{"location":"#features","text":"MicroRaft is a complete implementation of the Raft consensus algorithm. It implements the leader election, log replication, log compaction (snapshotting), and cluster membership changes components. Additionally, it realizes a rich set of optimizations and enhancements, as listed below, to allow developers to run Raft clusters in a reliable and performant manner, and tune its behaviour based on their needs. Adaptive batching during log replication Back pressure to prevent OOMEs on Raft leader and followers Parallel snapshot transfer from Raft leader and followers Pre-voting and leader stickiness ( \u00a7 4.2.3 and 9.6 of the Raft dissertation , and 4 Modifications for Raft Consensus ) Auto-demotion of Raft leader on loss of quorum heartbeats Linearizable quorum reads without appending log entries (\u00a7 6.4 of the Raft dissertation) Lease-based local queries on Raft leader (\u00a7 6.4.1 of the Raft dissertation) Monotonic local queries on Raft followers (\u00a7 6.4.1 of the Raft dissertation) Parallel disk writes on Raft leader and followers (\u00a7 10.2.1 of the Raft dissertation) Leadership transfer (\u00a7 3.10 of the Raft dissertation) Improved majority quorums","title":"Features"},{"location":"#use-cases","text":"MicroRaft can be used for building highly available and strongly consistent data, metadata and coordination services. An example of data service is a distributed key-value store. You can build a distributed key-value store where each partition / shard is maintained by a separate Raft cluster ( Raft group in MicroRaft terms). MicroRaft can be also used for building a control plane or coordination cluster. It can store the metadata of your large-scale data services. High-level APIs, such as leader election mechanisms, group membership management systems, distributed locks, distributed transaction managers, or distributed resource schedulers can be also built on top of MicroRaft. Please note that MicroRaft is not a high-level solution like a distributed key-value store, or a distributed lock service. It is a library that offers a set of abstractions and functionalities to help you build such high-level systems without intertwining your system with Raft code.","title":"Use cases"},{"location":"#getting-started","text":"Just run the following command on your terminal for a sneak peek at MicroRaft. It starts a 3-node local Raft group, elects a leader, and commits a number of operations. $ gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial If you want to learn more about how to use MicroRaft for building a CP distributed system, you can check out the Main Abstractions section first, and then read the tutorial to build an atomic register on top of MicroRaft.","title":"Getting started"},{"location":"#getting-involved","text":"MicroRaft is a new open-source library. Your contribution and feedback is welcome! The development happens on Github . You can follow @MicroRaft on Twitter for announcements.","title":"Getting involved"},{"location":"#what-is-consensus","text":"Consensus is one of the fundamental problems in distributed systems. It involves multiple servers agree on a value. Once a value is decided, the decision is final. Consensus algorithms are very useful in a plethora of distributed systems that require high availability and strong consistency. Paxos, first introduced by Leslie Lamport, is probably the most widely known consensus algorithm. However, it has been also known as difficult to reason about and lacking details for building practical implementations. Raft was introduced in 2013 as a new consensus algorithm with the main goal of understandability. Ever since its introduction, Raft has received widespread adoption in the industry. Raft approaches the consensus problem in the context of replicated state machines, where a group of servers applies the same set of operations and computes identical copies of the same state. Raft's primary enabler of understandability is the problem decomposition technique. It divides the consensus problem into 3 pieces: leader election, log replication and safety, and solves each piece relatively independently. Raft starts by electing a leader. There is a single functional leader managing the servers, and upon its failure a new leader is elected. Each server keeps a local log. Clients send their requests to the leader. The leader appends incoming requests into its log and replicates them to the other servers. Each server appends the requests sent by the leader into its log. Once a request is appended to the local logs of sufficient number (i.e., more than half) of servers, the leader considers the request committed, hence executes it on its local state machine, also notifies other servers to do the same. Raft orders requests by the indices they are appended to the replicated log. In addition, Raft's leader election and log replication rules ensure that once a request is committed and executed at a given log index on one server, no other server can execute another request for the same log index, including the presence of non-Byzantine failures. This is basically Raft's safety property. Thanks to this property, each server executes the same sequence of requests. Once these requests are deterministic, servers compute identical copies of the same state and produce the same output values. For more details about Raft, please see the In Search of an Understandable Consensus Algorithm paper by Diego Ongaro and John Ousterhout.","title":"What is consensus?"},{"location":"#acknowledgements","text":"MicroRaft originates from Hazelcast IMDG's Raft implementation and includes several significant improvements on the public APIs and internals. MicroRaft's logo is created by modifying Raft's original logo . Raft's logo was created by Andrea Ruygt and licensed under the Creative Commons Attribution-4.0 International .","title":"Acknowledgements"},{"location":"blog/","text":"Blog September 3, 2020 | Introducing MicroRaft In this blog post I announce the first public release of MicroRaft.","title":"Blog"},{"location":"blog/#blog","text":"September 3, 2020 | Introducing MicroRaft In this blog post I announce the first public release of MicroRaft.","title":"Blog"},{"location":"blog/2021-09-03-introducing-microraft/","text":"Introducing MicroRaft September 3, 2021 | Ensar Basri Kahveci Disclaimer: MicroRaft is a project I develop in my free time. It is not affiliated, associated, endorsed by, or in any way officially connected with my current employer Facebook, or any of its subsidiaries or its affiliates. I am pleased to announce the first public release of MicroRaft! MicroRaft is an open-source implementation of the Raft consensus algorithm in Java. You can use MicroRaft to build highly available and strongly consistent data, metadata and coordination services. The source code is available at Github with the Apache 2 License. MicroRaft is a complete implementation of the Raft consensus algorithm. It implements the leader election, log replication, log compaction (snapshotting), and cluster membership change components. Additionally, it realizes a rich set of optimizations and enhancements to allow developers to run Raft clusters in a reliable and performant manner, and tune its behaviour based on their needs. MicroRaft works on top of a minimalistic and modular design. It is a single lightweight JAR with a few hundred KBs of size and only a logging dependency. It contains an isolated implementation of the Raft consensus algorithm, and a set of accompanying interfaces to run the algorithm in a multi-threaded and distributed environment. These interfaces surround the Raft consensus algorithm, and abstract away the concerns of persistence, thread-safety, serialization, networking and actual state machine logic. Developers are required to implement these interfaces to build CP distributed systems on top of MicroRaft. Use cases You can use MicroRaft to build highly available and strongly consistent data, metadata and coordination services. Here are some common examples: distributed key-value store where each partition / shard is maintained by a separate Raft cluster control plane or coordination cluster leader election mechanisms group membership management systems distributed locks distributed transaction managers distributed resource schedulers Main Abstractions The following figure depicts MicroRaft's main abstractions. RaftNode runs the Raft consensus algorithm with the Actor model. Clients talk to the leader RaftNode in order to replicate their operations. They can talk to either the leader or follower RaftNode s to run queries with different consistency guarantees. RaftNode uses RaftModelFactory to create Raft log entries, snapshot entries, and Raft RPC request and response objects. These objects are defined under an interface, RaftModel , so that developers can implement them using their favorite serialization framework, such as Protocol Buffers, Avro, or Thrift. Each RaftNode talks to its own Transport object to communicate with the other RaftNode s. For instance, if the RaftModel interfaces are implemented with Protocol buffers, a Transport implementation can internally use gRPC to communicate with the other RaftNode s. RaftNode also uses RaftStore to persist Raft log entries and StateMachine snapshots to stable storage. StateMachine enables users to implement arbitrary services, such as an atomic register or a key-value store, and execute operations on them. If it is a key-value store, then clients can replicate operations like get , set , delete , etc. Once a log entry is committed, i.e, successfully replicated to the majority of the Raft group, the operation it contains is passed to StateMachine for execution and the leader RaftNode returns the output of the execution to the client. Getting started Run the following command on your terminal for a sneak peek at MicroRaft. It starts a 3-node local Raft group (a Raft cluster in MicroRaft terms), elects a leader, and commits a number of operations. $ gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial Follow the tutorial to learn how to build an atomic register on top of MicroRaft and for full details, check out MicroRaft's APIs and main abstractions . Ode to open source I couldn't have brought MicroRaft into the daylight without the power of open source. MicroRaft originates from the Raft implementation that powers Hazelcast IMDG's CP Subsystem module. I was one of the main contributors of the CP Subsystem and was thinking about converting its Raft code into a separate library back in 2019, but didn't have the time to try my idea before leaving Hazelcast in February 2020. I decided to give this project a try to amuse myself during the lockdown while I was still in Turkey. After I relocated to London, I was too busy with everything related to starting a new life and a new job in a new country, so I needed a whole year to find some free time and make the project ready for release. Mehmet Dogan and I developed the original Raft code inside Hazelcast codebase, but we isolated it from the rest of the Hazelcast code. It depends on Hazelcast for networking, logging and testing. So I started by moving out the Raft code and defining abstractions for the parts depending on Hazelcast. Then I implemented several significant enhancements and improvements that you can see at the commit history . MicroRaft proudly carries on Hazelcast's open-source heritage and is released with the Apache 2 License. What is next I wrote down a list of future work on MicroRaft. I am planning to work on them in my free time. The list is tentative and there is nothing urgent at the moment. MicroRaft is a new open source project. Any kind of contribution and feedback is welcome! The development happens on Github . Last, you can follow @MicroRaft on Twitter for announcements.","title":"September 3, 2021 | Introducing MicroRaft"},{"location":"blog/2021-09-03-introducing-microraft/#introducing-microraft","text":"September 3, 2021 | Ensar Basri Kahveci Disclaimer: MicroRaft is a project I develop in my free time. It is not affiliated, associated, endorsed by, or in any way officially connected with my current employer Facebook, or any of its subsidiaries or its affiliates. I am pleased to announce the first public release of MicroRaft! MicroRaft is an open-source implementation of the Raft consensus algorithm in Java. You can use MicroRaft to build highly available and strongly consistent data, metadata and coordination services. The source code is available at Github with the Apache 2 License. MicroRaft is a complete implementation of the Raft consensus algorithm. It implements the leader election, log replication, log compaction (snapshotting), and cluster membership change components. Additionally, it realizes a rich set of optimizations and enhancements to allow developers to run Raft clusters in a reliable and performant manner, and tune its behaviour based on their needs. MicroRaft works on top of a minimalistic and modular design. It is a single lightweight JAR with a few hundred KBs of size and only a logging dependency. It contains an isolated implementation of the Raft consensus algorithm, and a set of accompanying interfaces to run the algorithm in a multi-threaded and distributed environment. These interfaces surround the Raft consensus algorithm, and abstract away the concerns of persistence, thread-safety, serialization, networking and actual state machine logic. Developers are required to implement these interfaces to build CP distributed systems on top of MicroRaft.","title":"Introducing MicroRaft"},{"location":"blog/2021-09-03-introducing-microraft/#use-cases","text":"You can use MicroRaft to build highly available and strongly consistent data, metadata and coordination services. Here are some common examples: distributed key-value store where each partition / shard is maintained by a separate Raft cluster control plane or coordination cluster leader election mechanisms group membership management systems distributed locks distributed transaction managers distributed resource schedulers","title":"Use cases"},{"location":"blog/2021-09-03-introducing-microraft/#main-abstractions","text":"The following figure depicts MicroRaft's main abstractions. RaftNode runs the Raft consensus algorithm with the Actor model. Clients talk to the leader RaftNode in order to replicate their operations. They can talk to either the leader or follower RaftNode s to run queries with different consistency guarantees. RaftNode uses RaftModelFactory to create Raft log entries, snapshot entries, and Raft RPC request and response objects. These objects are defined under an interface, RaftModel , so that developers can implement them using their favorite serialization framework, such as Protocol Buffers, Avro, or Thrift. Each RaftNode talks to its own Transport object to communicate with the other RaftNode s. For instance, if the RaftModel interfaces are implemented with Protocol buffers, a Transport implementation can internally use gRPC to communicate with the other RaftNode s. RaftNode also uses RaftStore to persist Raft log entries and StateMachine snapshots to stable storage. StateMachine enables users to implement arbitrary services, such as an atomic register or a key-value store, and execute operations on them. If it is a key-value store, then clients can replicate operations like get , set , delete , etc. Once a log entry is committed, i.e, successfully replicated to the majority of the Raft group, the operation it contains is passed to StateMachine for execution and the leader RaftNode returns the output of the execution to the client.","title":"Main Abstractions"},{"location":"blog/2021-09-03-introducing-microraft/#getting-started","text":"Run the following command on your terminal for a sneak peek at MicroRaft. It starts a 3-node local Raft group (a Raft cluster in MicroRaft terms), elects a leader, and commits a number of operations. $ gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial Follow the tutorial to learn how to build an atomic register on top of MicroRaft and for full details, check out MicroRaft's APIs and main abstractions .","title":"Getting started"},{"location":"blog/2021-09-03-introducing-microraft/#ode-to-open-source","text":"I couldn't have brought MicroRaft into the daylight without the power of open source. MicroRaft originates from the Raft implementation that powers Hazelcast IMDG's CP Subsystem module. I was one of the main contributors of the CP Subsystem and was thinking about converting its Raft code into a separate library back in 2019, but didn't have the time to try my idea before leaving Hazelcast in February 2020. I decided to give this project a try to amuse myself during the lockdown while I was still in Turkey. After I relocated to London, I was too busy with everything related to starting a new life and a new job in a new country, so I needed a whole year to find some free time and make the project ready for release. Mehmet Dogan and I developed the original Raft code inside Hazelcast codebase, but we isolated it from the rest of the Hazelcast code. It depends on Hazelcast for networking, logging and testing. So I started by moving out the Raft code and defining abstractions for the parts depending on Hazelcast. Then I implemented several significant enhancements and improvements that you can see at the commit history . MicroRaft proudly carries on Hazelcast's open-source heritage and is released with the Apache 2 License.","title":"Ode to open source"},{"location":"blog/2021-09-03-introducing-microraft/#what-is-next","text":"I wrote down a list of future work on MicroRaft. I am planning to work on them in my free time. The list is tentative and there is nothing urgent at the moment. MicroRaft is a new open source project. Any kind of contribution and feedback is welcome! The development happens on Github . Last, you can follow @MicroRaft on Twitter for announcements.","title":"What is next"},{"location":"docs/configuration/","text":"Configuration MicroRaft is a lightweight library with a minimal feature set, yet it allows users to fine-tune its behaviour. In this section, we elaborate the configuration parameters available in MicroRaft. RaftConfig is an immutable configuration class containing a number of parameters to tune behaviour of your Raft nodes. You can populate it via RaftConfigBuilder . Your software in which MicroRaft is embedded could be already working with HOCON or YAML files. MicroRaft also offers HOCON and YAML parsers to populate RaftConfig objects for such cases. Once you create a RaftConfig object either programmatically, or by parsing a HOCON or YAML file, you can provide it to RaftNodeBuilder while building RaftNode instances. You can see MicroRaft's configuration parameters below: Leader election timeout milliseconds: Duration of leader election rounds in milliseconds. If a candidate cannot win majority votes before this timeout elapses, a new leader election round is started. See \"\u00a7 5.2: Leader Election\" in the Raft paper ) for more details about how leader elections work in Raft. Leader heartbeat timeout seconds: Duration in seconds for a follower to decide on failure of the current leader and start a new leader election round. If this duration is too short, a leader could be considered as failed unnecessarily in case of a small hiccup. If it is too long, it takes longer to detect an actual failure. The Raft paper uses a single parameter, election timeout , to both detect failure of the leader and perform a leader election round. A follower decides the current leader to be crashed if the election timeout elapses after the last received AppendEntriesRPC . Moreover, a candidate starts a new leader election round if the election timeout elapses before it acquires the majority votes or another candidate announces the leadership. The Raft paper discusses the election timeout to be configured around 500 milliseconds. Even though such a low timeout value works just fine for leader elections, we have experienced that it causes false positives for failure detection of the leader when there is a hiccup in the system. When a leader slows down temporarily for any reason, its followers may start a new leader election round very quickly. To prevent such problems and allow users to tune availability of the system with more granularity, MicroRaft uses another parameter, leader heartbeat timeout , to detect failure of the leader. Please note that having 2 timeout parameters does not make any difference in terms of the safety property. If both values are configured the same, MicroRaft's behaviour becomes identical to the behaviour described in the Raft paper. Leader heartbeat period seconds: Duration in seconds for a Raft leader node to send periodic heartbeat requests to its followers in order to denote its liveliness. Periodic heartbeat requests are actually append entries requests and can contain log entries. A periodic heartbeat request is not sent to a follower if an append entries request has been sent to that follower recently. Maximum pending log entry count: Maximum number of pending log entries in the leader's Raft log before temporarily rejecting new requests of clients. This configuration enables a back pressure mechanism to prevent OOME when a Raft leader cannot keep up with the requests sent by the clients. When the pending log entries buffer whose capacity is specified with this configuration field is filled, new requests fail with CannotReplicateException to slow down clients. You can configure this field by considering the degree of concurrency of your clients. Append entries request batch size: In MicroRaft, a leader Raft node sends log entries to its followers in batches to improve the throughput. This configuration parameter specifies the maximum number of Raft log entries that can be sent as a batch in a single append entries request. Commit count to take snapshot: Number of new commits to initiate a new snapshot after the last snapshot taken by a Raft node. This value must be configured wisely as it effects performance of the system in multiple ways. If a small value is set, it means that snapshots are taken too frequently and Raft nodes keep a very short Raft log. If snapshot objects are large and the Raft state is persisted to disk, this can create an unnecessary overhead on IO performance. Moreover, a Raft leader can send too many snapshots to slow followers which can create a network overhead. On the other hand, if a very large value is set, it can create a memory overhead since Raft log entries are going to be kept in memory until the next snapshot. Transfer snapshots from followers: MicroRaft's Raft log design ensures that every Raft node takes a snapshot at exactly the same log index. This behaviour enables an optimization. When a follower falls far behind the Raft leader and needs to install a snapshot, it transfers the snapshot chunks from both the Raft leader and other followers in parallel. By this way, we utilize the bandwidth of the followers to reduce the load on the leader and speed up the snapshot transfer process. Raft node report publish period seconds: It denotes how frequently a Raft node publishes a report of its internal Raft state. RaftNodeReport objects can be used for monitoring a running Raft group. HOCON Configuration RaftConfig objects can be populated from HOCON files easily if you add the microraft-hocon dependency to your classpath. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-hocon</artifactId> <version>0.1</version> </dependency> You can see a HOCON config string below: raft { leader-election-timeout-millis: 1000 leader-heartbeat-timeout-secs: 10 leader-heartbeat-period-secs: 2 max-pending-log-entry-count: 5000 append-entries-request-batch-size: 1000 commit-count-to-take-snapshot: 50000 transfer-snapshots-from-followers-enabled: true raft-node-report-publish-period-secs: 10 } You can parse a HOCON file as shown below: String configFilePath = \"...\"; Config hoconConfig = ConfigFactory.parseFile(new File(configFilePath)); RaftConfig raftConfig = HoconRaftConfigParser.parseConfig(hoconConfig); Please refer to MicroRaft HOCON project for details. YAML Configuration Similarly, RaftConfig objects can be populated from YAML files easily if you add the microraft-yaml dependency to your classpath. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-yaml</artifactId> <version>0.1</version> </dependency> You can see a YAML config string below: raft: leader-election-timeout-millis: 1000 leader-heartbeat-timeout-secs: 10 leader-heartbeat-period-secs: 2 max-pending-log-entry-count: 5000 append-entries-request-batch-size: 1000 commit-count-to-take-snapshot: 50000 transfer-snapshots-from-followers-enabled: true raft-node-report-publish-period-secs: 10 You can parse a YAML file as shown below: String configFilePath = \"...\"; RaftConfig raftConfig = YamlRaftConfigParser.parseFile(new Yaml(), configFilePath); Please refer to MicroRaft YAML project for details.","title":"Configuration"},{"location":"docs/configuration/#configuration","text":"MicroRaft is a lightweight library with a minimal feature set, yet it allows users to fine-tune its behaviour. In this section, we elaborate the configuration parameters available in MicroRaft. RaftConfig is an immutable configuration class containing a number of parameters to tune behaviour of your Raft nodes. You can populate it via RaftConfigBuilder . Your software in which MicroRaft is embedded could be already working with HOCON or YAML files. MicroRaft also offers HOCON and YAML parsers to populate RaftConfig objects for such cases. Once you create a RaftConfig object either programmatically, or by parsing a HOCON or YAML file, you can provide it to RaftNodeBuilder while building RaftNode instances. You can see MicroRaft's configuration parameters below: Leader election timeout milliseconds: Duration of leader election rounds in milliseconds. If a candidate cannot win majority votes before this timeout elapses, a new leader election round is started. See \"\u00a7 5.2: Leader Election\" in the Raft paper ) for more details about how leader elections work in Raft. Leader heartbeat timeout seconds: Duration in seconds for a follower to decide on failure of the current leader and start a new leader election round. If this duration is too short, a leader could be considered as failed unnecessarily in case of a small hiccup. If it is too long, it takes longer to detect an actual failure. The Raft paper uses a single parameter, election timeout , to both detect failure of the leader and perform a leader election round. A follower decides the current leader to be crashed if the election timeout elapses after the last received AppendEntriesRPC . Moreover, a candidate starts a new leader election round if the election timeout elapses before it acquires the majority votes or another candidate announces the leadership. The Raft paper discusses the election timeout to be configured around 500 milliseconds. Even though such a low timeout value works just fine for leader elections, we have experienced that it causes false positives for failure detection of the leader when there is a hiccup in the system. When a leader slows down temporarily for any reason, its followers may start a new leader election round very quickly. To prevent such problems and allow users to tune availability of the system with more granularity, MicroRaft uses another parameter, leader heartbeat timeout , to detect failure of the leader. Please note that having 2 timeout parameters does not make any difference in terms of the safety property. If both values are configured the same, MicroRaft's behaviour becomes identical to the behaviour described in the Raft paper. Leader heartbeat period seconds: Duration in seconds for a Raft leader node to send periodic heartbeat requests to its followers in order to denote its liveliness. Periodic heartbeat requests are actually append entries requests and can contain log entries. A periodic heartbeat request is not sent to a follower if an append entries request has been sent to that follower recently. Maximum pending log entry count: Maximum number of pending log entries in the leader's Raft log before temporarily rejecting new requests of clients. This configuration enables a back pressure mechanism to prevent OOME when a Raft leader cannot keep up with the requests sent by the clients. When the pending log entries buffer whose capacity is specified with this configuration field is filled, new requests fail with CannotReplicateException to slow down clients. You can configure this field by considering the degree of concurrency of your clients. Append entries request batch size: In MicroRaft, a leader Raft node sends log entries to its followers in batches to improve the throughput. This configuration parameter specifies the maximum number of Raft log entries that can be sent as a batch in a single append entries request. Commit count to take snapshot: Number of new commits to initiate a new snapshot after the last snapshot taken by a Raft node. This value must be configured wisely as it effects performance of the system in multiple ways. If a small value is set, it means that snapshots are taken too frequently and Raft nodes keep a very short Raft log. If snapshot objects are large and the Raft state is persisted to disk, this can create an unnecessary overhead on IO performance. Moreover, a Raft leader can send too many snapshots to slow followers which can create a network overhead. On the other hand, if a very large value is set, it can create a memory overhead since Raft log entries are going to be kept in memory until the next snapshot. Transfer snapshots from followers: MicroRaft's Raft log design ensures that every Raft node takes a snapshot at exactly the same log index. This behaviour enables an optimization. When a follower falls far behind the Raft leader and needs to install a snapshot, it transfers the snapshot chunks from both the Raft leader and other followers in parallel. By this way, we utilize the bandwidth of the followers to reduce the load on the leader and speed up the snapshot transfer process. Raft node report publish period seconds: It denotes how frequently a Raft node publishes a report of its internal Raft state. RaftNodeReport objects can be used for monitoring a running Raft group.","title":"Configuration"},{"location":"docs/configuration/#hocon-configuration","text":"RaftConfig objects can be populated from HOCON files easily if you add the microraft-hocon dependency to your classpath. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-hocon</artifactId> <version>0.1</version> </dependency> You can see a HOCON config string below: raft { leader-election-timeout-millis: 1000 leader-heartbeat-timeout-secs: 10 leader-heartbeat-period-secs: 2 max-pending-log-entry-count: 5000 append-entries-request-batch-size: 1000 commit-count-to-take-snapshot: 50000 transfer-snapshots-from-followers-enabled: true raft-node-report-publish-period-secs: 10 } You can parse a HOCON file as shown below: String configFilePath = \"...\"; Config hoconConfig = ConfigFactory.parseFile(new File(configFilePath)); RaftConfig raftConfig = HoconRaftConfigParser.parseConfig(hoconConfig); Please refer to MicroRaft HOCON project for details.","title":"HOCON Configuration"},{"location":"docs/configuration/#yaml-configuration","text":"Similarly, RaftConfig objects can be populated from YAML files easily if you add the microraft-yaml dependency to your classpath. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-yaml</artifactId> <version>0.1</version> </dependency> You can see a YAML config string below: raft: leader-election-timeout-millis: 1000 leader-heartbeat-timeout-secs: 10 leader-heartbeat-period-secs: 2 max-pending-log-entry-count: 5000 append-entries-request-batch-size: 1000 commit-count-to-take-snapshot: 50000 transfer-snapshots-from-followers-enabled: true raft-node-report-publish-period-secs: 10 You can parse a YAML file as shown below: String configFilePath = \"...\"; RaftConfig raftConfig = YamlRaftConfigParser.parseFile(new Yaml(), configFilePath); Please refer to MicroRaft YAML project for details.","title":"YAML Configuration"},{"location":"docs/main-abstractions/","text":"Main Abstractions MicroRaft's main abstractions are listed below. RaftConfig RaftConfig contains configuration options related to the Raft consensus algorithm and MicroRaft's implementation. Please check the Configuration section for details. RaftNode RaftNode runs the Raft consensus algorithm as a member of a Raft group. A Raft group is a cluster of RaftNode instances that behave as a replicated state machine . RaftNode contains APIs for replicating operations, performing queries, applying membership changes in the Raft group, handling Raft RPCs and responses, etc. Raft nodes are identified by [group id, node id] pairs. Multiple Raft groups can run in the same environment, distributed or even in a single JVM process, and they can be discriminated from each other with unique group ids. A single JVM process can run multiple Raft nodes that belong to different Raft groups or even the same Raft group. RaftNode s execute the Raft consensus algorithm with the Actor model . In this model, each RaftNode runs in a single-threaded manner. It uses a RaftNodeExecutor to sequentially handle API calls and RaftMessage objects sent by other RaftNode s. The communication between RaftNode s are implemented with the message-passing approach and abstracted away with the Transport interface. In addition to these interfaces that abstract away task execution and networking, RaftNode s use StateMachine objects to execute queries and committed operations, and RaftStore objects to persist internal Raft state to stable storage to be able to recover from crashes. All of these abstractions are explained below. RaftEndpoint RaftEndpoint represents an endpoint that participates to at least one Raft group and executes the Raft consensus algorithm with a RaftNode . RaftNode differentiates members of a Raft group with a unique id. MicroRaft users need to provide a unique id for each RaftEndpoint . Other than this information, RaftEndpoint implementations can contain custom fields, such as network addresses and tags, to be utilized by Transport implementations. RaftRole RaftRole denotes the roles of RaftNode s as specified in the Raft consensus algorithm. Currently, MicroRaft implements the main roles defined in the paper: LEADER , CANDIDATE , and FOLLOWER . The popular extension roles, such as LEARNER and WITNESS , are not implemented yet, but they are on the roadmap. RaftNodeStatus RaftNodeStatus denotes the statuses of a RaftNode during its own and its Raft group's lifecycle. A RaftNode is in the INITIAL status when it is created, and moves to the ACTIVE status when it is started with a RaftNode.start() call. It stays in this status until either a membership change is triggered in the Raft group, or either the Raft group or Raft node is terminated. StateMachine StateMachine enables users to implement arbitrary services, such as an atomic register or a key-value store, and execute operations on them. Currently, MicroRaft supports memory-based state machines with datasets in the gigabytes. RaftNode does not deal with the actual logic of committed operations. Once a given operation is committed with the Raft consensus algorithm, i.e., it is replicated to the majority of the Raft group, the operation is passed to the provided StateMachine implementation. It is the StateMachine implementation's responsibility to ensure deterministic execution of committed operations. Since RaftNodeExecutor ensures the thread-safe execution of the tasks submitted by RaftNode s, which include actual execution of committed user-supplied operations, StateMachine implementations do not need to be thread-safe. RaftModel and RaftModelFactory RaftModel is the base interface for the objects that hit network and disk. There are 2 other interfaces extending this interface: BaseLogEntry and RaftMessage . BaseLogEntry is used for representing log and snapshot entries stored in the Raft log. RaftMessage is used for Raft RPCs and their responses. Please see the interfaces inside io.microraft.model for more details. In addition, there is a RaftModelFactory interface for creating RaftModel objects with the builder pattern. MicroRaft comes with a default POJO-style implementation of these interfaces available under the io.microraft.model.impl package. Users of MicroRaft can define serialization & deserialization strategies for the default implementation objects, or implement the RaftModel interfaces with a serialization framework, such as Protocol Buffers . Transport Transport is used for communicating Raft nodes with each other. Transport implementations must be able to serialize RaftMessage objects created by RaftModelFactory . MicroRaft requires a minimum set of functionality for networking. There are only two methods in this interface, one for sending a RaftMessage to a RaftEndpoint and another one for checking reachability of a RaftEndpoint . RaftStore and RestoredRaftState RaftStore is used for persisting the internal state of the Raft consensus algorithm. Its implementations must provide the durability guarantees defined in the interface. If a RaftNode crashes, its persisted state could be read back from stable storage into a RestoredRaftState object and the RaftNode could be restored back. RestoredRaftState contains all the necessary information to recover RaftNode instances from crashes. RaftStore does not persist internal state of StateMachine implementations. Upon recovery, a RaftNode starts with an empty state of the state machine, discovers the current commit index and re-executes all the operations in the Raft log up to the commit index to re-populate the state machine. RaftNodeExecutor RaftNodeExecutor is used by RaftNode to execute the Raft consensus algorithm with the Actor model . A RaftNode runs by submitting tasks to its RaftNodeExecutor . All tasks submitted by a RaftNode must be executed serially, with maintaining the happens-before relationship , so that the Raft consensus algorithm and the user-provided state machine logic could be executed without synchronization. MicroRaft contains a default implementation, DefaultRaftNodeExecutor . It internally uses a single-threaded ScheduledExecutorService and should be suitable for most of the use-cases. Users of MicroRaft can provide their own RaftNodeExecutor implementations if they want to run RaftNode s in their own threading system according to the rules defined by MicroRaft. RaftException RaftException is the base class for Raft-related exceptions. MicroRaft defines a number of custom exceptions to report some certain failure scenarios to clients. How to run a Raft group In order to run a Raft group (i.e., Raft cluster) using MicroRaft, we need to: implement the RaftEndpoint interface to identify RaftNode s we are going to run, provide a StateMachine implementation for the actual state machine logic (key-value store, atomic register, etc.), (optional) implement the RaftModel and RaftModelFactory interfaces to create Raft RPC request / response objects and Raft log entries, or simply use the default POJO-style implementation of these interfaces available under the io.microraft.model.impl package, provide a Transport implementation to realize serialization of RaftModel objects and networking, (optional) provide a RaftStore implementation if we want to restore crashed Raft nodes. We can persist the internal Raft node state to stable storage via the RaftStore interface and recover from Raft node crashes by restoring persisted Raft state. Otherwise, we could use the already-existing NopRaftStore utility which makes the internal Raft state volatile and disables crash-recovery. If we don't implement persistence, crashed Raft nodes cannot be restarted and need to be removed from the Raft group to not to damage availability. Note that the lack of persistence limits the overall fault tolerance capabilities of Raft groups. Please refer to the Resiliency and Fault Tolerance section to learn more about MicroRaft's fault tolerance capabilities. build a discovery and RPC mechanism to replicate and commit operations on a Raft group. This is required because simplicity is the primary concern for MicroRaft's design philosophy. MicroRaft offers a minimum API set to cover the fundamental functionality and enables its users to implement higher-level abstractions, such as an RPC system with request routing, retries, and deduplication. For instance, MicroRaft neither broadcasts the Raft group members to any external discovery system, nor integrates with any observability tool, but it exposes all necessary information, such as Raft group members, leader Raft endpoint, term, commit index, via RaftNodeReport , which can be accessed via RaftNode.getReport() and RaftNodeReportListener . We can use that information to feed our discovery services and monitoring tools. Similarly, the public APIs on RaftNode do not employ request routing or retry mechanisms. For instance, if a client tries to replicate an operation via a follower or a candidate RaftNode , RaftNode responds back with a NotLeaderException , instead of internally forwarding the operation to the leader RaftNode . NotLeaderException contains RaftEndpoint of the current leader RaftNode so that clients can send their requests to it. In the next section, we will build an atomic register on top of MicroRaft to demonstrate how to implement and use MicroRaft's main abstractions. Architectural overview of a Raft group The following figure depicts an architectural overview of a Raft group based on the main abstractions explained above. Clients talk to the leader RaftNode for replicating operations. They can talk to both the leader and follower RaftNode s for running queries with different consistency guarantees. RaftNode uses RaftModelFactory to create Raft log entries, snapshot entries, and Raft RPC request and response objects. Each RaftNode uses its Transport object to communicate with the other RaftNode s. It also uses RaftStore to persist Raft log entries and snapshots to stable storage. Last, once a log entry is committed, i.e, it is successfully replicated to the majority of the Raft group, its operation is passed to StateMachine for execution, and output of the execution is returned to the client by the leader RaftNode . What is next? In the next section , we will build an atomic register on top of MicroRaft.","title":"Main Abstractions"},{"location":"docs/main-abstractions/#main-abstractions","text":"MicroRaft's main abstractions are listed below.","title":"Main Abstractions"},{"location":"docs/main-abstractions/#raftconfig","text":"RaftConfig contains configuration options related to the Raft consensus algorithm and MicroRaft's implementation. Please check the Configuration section for details.","title":"RaftConfig"},{"location":"docs/main-abstractions/#raftnode","text":"RaftNode runs the Raft consensus algorithm as a member of a Raft group. A Raft group is a cluster of RaftNode instances that behave as a replicated state machine . RaftNode contains APIs for replicating operations, performing queries, applying membership changes in the Raft group, handling Raft RPCs and responses, etc. Raft nodes are identified by [group id, node id] pairs. Multiple Raft groups can run in the same environment, distributed or even in a single JVM process, and they can be discriminated from each other with unique group ids. A single JVM process can run multiple Raft nodes that belong to different Raft groups or even the same Raft group. RaftNode s execute the Raft consensus algorithm with the Actor model . In this model, each RaftNode runs in a single-threaded manner. It uses a RaftNodeExecutor to sequentially handle API calls and RaftMessage objects sent by other RaftNode s. The communication between RaftNode s are implemented with the message-passing approach and abstracted away with the Transport interface. In addition to these interfaces that abstract away task execution and networking, RaftNode s use StateMachine objects to execute queries and committed operations, and RaftStore objects to persist internal Raft state to stable storage to be able to recover from crashes. All of these abstractions are explained below.","title":"RaftNode"},{"location":"docs/main-abstractions/#raftendpoint","text":"RaftEndpoint represents an endpoint that participates to at least one Raft group and executes the Raft consensus algorithm with a RaftNode . RaftNode differentiates members of a Raft group with a unique id. MicroRaft users need to provide a unique id for each RaftEndpoint . Other than this information, RaftEndpoint implementations can contain custom fields, such as network addresses and tags, to be utilized by Transport implementations.","title":"RaftEndpoint"},{"location":"docs/main-abstractions/#raftrole","text":"RaftRole denotes the roles of RaftNode s as specified in the Raft consensus algorithm. Currently, MicroRaft implements the main roles defined in the paper: LEADER , CANDIDATE , and FOLLOWER . The popular extension roles, such as LEARNER and WITNESS , are not implemented yet, but they are on the roadmap.","title":"RaftRole"},{"location":"docs/main-abstractions/#raftnodestatus","text":"RaftNodeStatus denotes the statuses of a RaftNode during its own and its Raft group's lifecycle. A RaftNode is in the INITIAL status when it is created, and moves to the ACTIVE status when it is started with a RaftNode.start() call. It stays in this status until either a membership change is triggered in the Raft group, or either the Raft group or Raft node is terminated.","title":"RaftNodeStatus"},{"location":"docs/main-abstractions/#statemachine","text":"StateMachine enables users to implement arbitrary services, such as an atomic register or a key-value store, and execute operations on them. Currently, MicroRaft supports memory-based state machines with datasets in the gigabytes. RaftNode does not deal with the actual logic of committed operations. Once a given operation is committed with the Raft consensus algorithm, i.e., it is replicated to the majority of the Raft group, the operation is passed to the provided StateMachine implementation. It is the StateMachine implementation's responsibility to ensure deterministic execution of committed operations. Since RaftNodeExecutor ensures the thread-safe execution of the tasks submitted by RaftNode s, which include actual execution of committed user-supplied operations, StateMachine implementations do not need to be thread-safe.","title":"StateMachine"},{"location":"docs/main-abstractions/#raftmodel-and-raftmodelfactory","text":"RaftModel is the base interface for the objects that hit network and disk. There are 2 other interfaces extending this interface: BaseLogEntry and RaftMessage . BaseLogEntry is used for representing log and snapshot entries stored in the Raft log. RaftMessage is used for Raft RPCs and their responses. Please see the interfaces inside io.microraft.model for more details. In addition, there is a RaftModelFactory interface for creating RaftModel objects with the builder pattern. MicroRaft comes with a default POJO-style implementation of these interfaces available under the io.microraft.model.impl package. Users of MicroRaft can define serialization & deserialization strategies for the default implementation objects, or implement the RaftModel interfaces with a serialization framework, such as Protocol Buffers .","title":"RaftModel and RaftModelFactory"},{"location":"docs/main-abstractions/#transport","text":"Transport is used for communicating Raft nodes with each other. Transport implementations must be able to serialize RaftMessage objects created by RaftModelFactory . MicroRaft requires a minimum set of functionality for networking. There are only two methods in this interface, one for sending a RaftMessage to a RaftEndpoint and another one for checking reachability of a RaftEndpoint .","title":"Transport"},{"location":"docs/main-abstractions/#raftstore-and-restoredraftstate","text":"RaftStore is used for persisting the internal state of the Raft consensus algorithm. Its implementations must provide the durability guarantees defined in the interface. If a RaftNode crashes, its persisted state could be read back from stable storage into a RestoredRaftState object and the RaftNode could be restored back. RestoredRaftState contains all the necessary information to recover RaftNode instances from crashes. RaftStore does not persist internal state of StateMachine implementations. Upon recovery, a RaftNode starts with an empty state of the state machine, discovers the current commit index and re-executes all the operations in the Raft log up to the commit index to re-populate the state machine.","title":"RaftStore and RestoredRaftState"},{"location":"docs/main-abstractions/#raftnodeexecutor","text":"RaftNodeExecutor is used by RaftNode to execute the Raft consensus algorithm with the Actor model . A RaftNode runs by submitting tasks to its RaftNodeExecutor . All tasks submitted by a RaftNode must be executed serially, with maintaining the happens-before relationship , so that the Raft consensus algorithm and the user-provided state machine logic could be executed without synchronization. MicroRaft contains a default implementation, DefaultRaftNodeExecutor . It internally uses a single-threaded ScheduledExecutorService and should be suitable for most of the use-cases. Users of MicroRaft can provide their own RaftNodeExecutor implementations if they want to run RaftNode s in their own threading system according to the rules defined by MicroRaft.","title":"RaftNodeExecutor"},{"location":"docs/main-abstractions/#raftexception","text":"RaftException is the base class for Raft-related exceptions. MicroRaft defines a number of custom exceptions to report some certain failure scenarios to clients.","title":"RaftException"},{"location":"docs/main-abstractions/#how-to-run-a-raft-group","text":"In order to run a Raft group (i.e., Raft cluster) using MicroRaft, we need to: implement the RaftEndpoint interface to identify RaftNode s we are going to run, provide a StateMachine implementation for the actual state machine logic (key-value store, atomic register, etc.), (optional) implement the RaftModel and RaftModelFactory interfaces to create Raft RPC request / response objects and Raft log entries, or simply use the default POJO-style implementation of these interfaces available under the io.microraft.model.impl package, provide a Transport implementation to realize serialization of RaftModel objects and networking, (optional) provide a RaftStore implementation if we want to restore crashed Raft nodes. We can persist the internal Raft node state to stable storage via the RaftStore interface and recover from Raft node crashes by restoring persisted Raft state. Otherwise, we could use the already-existing NopRaftStore utility which makes the internal Raft state volatile and disables crash-recovery. If we don't implement persistence, crashed Raft nodes cannot be restarted and need to be removed from the Raft group to not to damage availability. Note that the lack of persistence limits the overall fault tolerance capabilities of Raft groups. Please refer to the Resiliency and Fault Tolerance section to learn more about MicroRaft's fault tolerance capabilities. build a discovery and RPC mechanism to replicate and commit operations on a Raft group. This is required because simplicity is the primary concern for MicroRaft's design philosophy. MicroRaft offers a minimum API set to cover the fundamental functionality and enables its users to implement higher-level abstractions, such as an RPC system with request routing, retries, and deduplication. For instance, MicroRaft neither broadcasts the Raft group members to any external discovery system, nor integrates with any observability tool, but it exposes all necessary information, such as Raft group members, leader Raft endpoint, term, commit index, via RaftNodeReport , which can be accessed via RaftNode.getReport() and RaftNodeReportListener . We can use that information to feed our discovery services and monitoring tools. Similarly, the public APIs on RaftNode do not employ request routing or retry mechanisms. For instance, if a client tries to replicate an operation via a follower or a candidate RaftNode , RaftNode responds back with a NotLeaderException , instead of internally forwarding the operation to the leader RaftNode . NotLeaderException contains RaftEndpoint of the current leader RaftNode so that clients can send their requests to it. In the next section, we will build an atomic register on top of MicroRaft to demonstrate how to implement and use MicroRaft's main abstractions.","title":"How to run a Raft group"},{"location":"docs/main-abstractions/#architectural-overview-of-a-raft-group","text":"The following figure depicts an architectural overview of a Raft group based on the main abstractions explained above. Clients talk to the leader RaftNode for replicating operations. They can talk to both the leader and follower RaftNode s for running queries with different consistency guarantees. RaftNode uses RaftModelFactory to create Raft log entries, snapshot entries, and Raft RPC request and response objects. Each RaftNode uses its Transport object to communicate with the other RaftNode s. It also uses RaftStore to persist Raft log entries and snapshots to stable storage. Last, once a log entry is committed, i.e, it is successfully replicated to the majority of the Raft group, its operation is passed to StateMachine for execution, and output of the execution is returned to the client by the leader RaftNode .","title":"Architectural overview of a Raft group"},{"location":"docs/main-abstractions/#what-is-next","text":"In the next section , we will build an atomic register on top of MicroRaft.","title":"What is next?"},{"location":"docs/monitoring/","text":"Monitoring RaftNodeReport contains detailed information about internal state of a RaftNode , such as its Raft role, term, leader, last log index, and commit index. We can feed our external monitoring systems with these information pieces as follows: We can build a simple pull-based system to query RaftNodeReport objects via RaftNode.getReport() and publish those objects to any external monitoring system. MicroRaft contains another abstraction, RaftNodeReportListener which is called by Raft nodes anytime there is an important change in the internal Raft state, such as leader change, term change, or snapshot installation. We can also use this abstraction to capture RaftNodeReport objects and notify external monitoring systems promptly with a push-based approach. Micrometer integration MicroRaft offers a module to publish Raft node metrics to external systems easily via the Micrometer project. Just add the following dependency to the classpath for using the Micrometer integration. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-metrics</artifactId> <version>0.1</version> </dependency> RaftNodeMetrics implements the RaftNodeReportListener interface and can be injected into created RaftNode instances via RaftNodeBuilder.setRaftNodeReportListener() . Then, several metrics extracted from published RaftNodeReport objects are passed to meter registries.","title":"Monitoring"},{"location":"docs/monitoring/#monitoring","text":"RaftNodeReport contains detailed information about internal state of a RaftNode , such as its Raft role, term, leader, last log index, and commit index. We can feed our external monitoring systems with these information pieces as follows: We can build a simple pull-based system to query RaftNodeReport objects via RaftNode.getReport() and publish those objects to any external monitoring system. MicroRaft contains another abstraction, RaftNodeReportListener which is called by Raft nodes anytime there is an important change in the internal Raft state, such as leader change, term change, or snapshot installation. We can also use this abstraction to capture RaftNodeReport objects and notify external monitoring systems promptly with a push-based approach.","title":"Monitoring"},{"location":"docs/monitoring/#micrometer-integration","text":"MicroRaft offers a module to publish Raft node metrics to external systems easily via the Micrometer project. Just add the following dependency to the classpath for using the Micrometer integration. <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-metrics</artifactId> <version>0.1</version> </dependency> RaftNodeMetrics implements the RaftNodeReportListener interface and can be injected into created RaftNode instances via RaftNodeBuilder.setRaftNodeReportListener() . Then, several metrics extracted from published RaftNodeReport objects are passed to meter registries.","title":"Micrometer integration"},{"location":"docs/release-notes/","text":"Release Notes 0.1 (January 18, 2021) MicroRaft is released.","title":"Release Notes"},{"location":"docs/release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"docs/release-notes/#01-january-18-2021","text":"MicroRaft is released.","title":"0.1 (January 18, 2021)"},{"location":"docs/resiliency-and-fault-tolerance/","text":"Resiliency and Fault Tolerance In this section, we will walk through different types of failure scenarios and discuss how MicroRaft handles each one of them. We will use MicroRaft's local testing utilities to demonstrate those failure scenarios. These utilities are mainly used for testing MicroRaft to a great extent without a distributed setting. Here, we will use them to run a Raft group in a single JVM process and inject different types of failures into the system. In terms of safety, the fundamental guarantee of the Raft consensus algorithm and hence MicroRaft is, operations are committed in a single global order, and a committed operation is never lost, as long as there is no Byzantine failure in the system. In MicroRaft, restarting a Raft node that has no persistence layer with the same identity or restarting it with a corrupted persistence state are examples of Byzantine failure. The availability of a Raft group mainly depends on if the majority (i.e., more than half) of the Raft nodes are alive and able to communicate with each other. The main rule is, 2f + 1 Raft nodes tolerate failure of f Raft nodes. For instance, a 3-node Raft group can tolerate failure of 1 Raft node, or a 5-node Raft group can tolerate failure of 2 Raft nodes without losing availability. 1. Handling high system load Even if the majority of a Raft group is alive, we may encounter unavailability issues if the Raft group is under high load and cannot keep up with the request rate. In this case, the leader Raft node temporarily stops accepting new requests and notifies the futures returned from the RaftNode methods with CannotReplicateException . This exception means that there are too many operations pending to be committed in the leader's local Raft log, or too many queries pending to be executed, so it temporarily rejects accepting new requests. Clients should apply some backoff before retrying their requests. We will demonstrate this scenario in a test below with a 3-node Raft group. In MicroRaft, a leader does not replicate log entries one by one. Instead, it keeps a buffer for incoming requests and replicates the log entries to the followers in batches in order to improve the throughput. Once this buffer is filled up, the leader stops accepting new requests. In this test, we allow the pending log entries buffer to keep at most 10 requests. We also slow down our followers synthetically by making them sleep for 3 seconds. Then, we start sending requests to the leader. After some time, our requests fail with CannotReplicateException . To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.HighLoadTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . 2. Minority failure Failure of the minority (i.e, less than half) may cause the Raft group to lose availability temporarily, but eventually the Raft group continues to accept and commit new requests. If we have a persistence implementation (i.e, RaftStore ), we can recover failed Raft nodes. On the other hand, if we don't have persistence or cannot recover the persisted Raft data, we can remove failed Raft nodes from the Raft group. Please note that when we remove a Raft node from a Raft group, the majority quorum size is re-calculated based on the new size of the Raft group. In order to replace a non-recoverable Raft node without hurting the overall availability of the Raft group, we should remove the crashed Raft node first and then add a fresh-new one. If Raft nodes are created without an actual RaftStore implementation in the beginning, restarting crashed Raft nodes with the same Raft endpoint identity breaks the safety of the Raft consensus algorithm. Therefore, when there is no persistence layer, the only recovery option for a failed Raft node is to remove it from the Raft group, which is possible only if the majority of the Raft group is up and running. To restart a crashed or terminated Raft node, we can read its persisted state into a RestoredRaftState object. Then, we can use this object to restore the Raft node back. Please note that terminating a Raft node manually without a persistence layer implementation is equivalent to a crash since there is no way to restore the Raft node back with its Raft state. MicroRaft provides a basic in-memory RaftStore implementation to enable crash-recovery testing. In the following code sample, we use this utility, i.e., InMemoryRaftStore , to demonstrate how to recover from Raft node failures. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.RestoreCrashedRaftNodeTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . This time we provide a factory object to enable LocalRaftGroup to create InMemoryRaftStore objects while configuring our Raft nodes. Hence, after we terminate our Raft nodes, we will be able to read their persisted state. Once we start the Raft group, we commit a value via the leader, observe that value with a local query on a follower, and crash a follower. Then, we read its persisted state via our InMemoryRaftStore object and restore the follower back. Please ignore the details of RaftTestUtils.getRestoredState() and RaftTestUtils.getRaftStore() . Once the follower starts running again, it talks to the other Raft nodes, discovers the current leader Raft node and its commit index, and replays all committed operations on its state machine. Our sysout lines in this test print the following: replicate result: value, commit index: 1 monotonic local query successful on follower. query result: value, commit index: 1 monotonic local query successful on restarted follower. query result: value, commit index: 1 When a Raft node starts with a restored Raft state, it discovers the current commit index and replays the Raft log, i.e., automatically applies all the log entries up to the commit index. We should be careful about operations that have side effects because the Raft log replay process triggers those side effects again. Please refer to the State Machine for more details. 3. Raft leader failure When a leader Raft node fails, the Raft group temporarily loses availability until the other Raft nodes notice the failure and elect a new leader. Delay of the detection of the leader's failure depends on the leader heartbeat timeout configuration. Please refer to the Configuration section to learn more about the leader election timeout and leader heartbeat timeout configuration parameters. If a client notices that the current leader is not responding, it can contact other Raft nodes in the Raft group in a round-robin fashion and query the leader via the RaftNode.getReport() API. If the leader actually crashes, the followers eventually notice its failure and elect a new leader. Then, our client will be able to discover the new leader Raft endpoint via this API. However, if a client cannot communicate with an alive leader because of an environmental issue, such as a network problem, it cannot replicate new operations, or run QueryPolicy.LINEARIZABLE and QueryPolicy.LEADER_LOCAL queries. It means that the Raft group is unavailable for this particular client. This is due to MicroRaft's simplicity-oriented design philosophy. In MicroRaft, when a follower Raft node receives an API call that requires the leadership role, it does not internally forward the call to the leader Raft node. Instead, it fails the call with NotLeaderException . Please note that this mechanism can be also used for leader discovery. When a client needs to discover the leader, it can try talking to any Raft node. If its call fails with NotLeaderException , the client can check if the exception points the current leader Raft endpoint via NotLeaderException.getLeader() . Otherwise, it can try the same with another Raft node. If a Raft leader crashes before a client receives response for an operation passed to RaftNode.replicate() , there are multiple possibilities: If the leader failed before replicating the operation to any follower, then the operation certainly won't be committed. If the failed leader replicated the operation to at least one follower, then the operation might be committed if a follower having that operation becomes leader. However, another follower could become the new leader and overwrite that operation if it was not replicated to the majority by the crashed leader. The good thing about queries is, they are idempotent. Clients can safely retry their queries on the new leader. It is up to the client to retry an operation whose result is not received, because a retry could cause the operation to be committed twice based on the actual failure scenario. MicroRaft goes for simplicity and does not employ deduplication (I have plans to implement an opt-in deduplication mechanism in future). If deduplication is needed, it can be done inside StateMachine implementations for now. We will see the second scenario described above in a code sample. In the following test, we replicate an operation via the Raft leader, but block the responses sent back from the followers. Even though the leader managed to replicate our operation to the majority, it is not able to commit it because it couldn't learn that the followers also appended this operation to their logs. At this step, we crash the leader. We won't get any response for our operation now since the leader is gone, so we will just re-replicate it with the new leader. The thing is, the previous leader managed to replicate our first operation to the majority, so the new leader will commit it. Since we replicate it for the second time with the new leader, we cause a duplicate commit. When we query the new leader, we see that there are 2 values applied to the state machine. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.RaftLeaderFailureTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Another trick could be designing our operations in an idempotent way and retry them automatically on leader failures, because duplicate commits do not make any harm for idempotent operations. However, it is not very easy to make every type of operation idempotent. 4. Majority failure Failure of the majority causes the Raft group to lose its availability and stop handling new requests. The only recovery option is to recover some of failed Raft nodes so that the majority becomes available again. Otherwise, the Raft group cannot be recovered. MicroRaft does not support any unsafe recovery policy for now. The duration of unavailability depends on how long the majority Raft nodes remain crashed. Clients won't be able to replicate any new operations or run linearizable queries in the meantime. However, we can still run local queries because QueryPolicy.EVENTUAL_CONSISTENCY does not require availability of the majority. In MicroRaft, on each heartbeat tick a leader Raft node checks if it is still in charge, i.e, it has received Append Entries RPC responses from majority quorum size - 1 (majority quorum size minus the leader itself) in the last leader heartbeat timeout period. For instance, in a 3-node Raft group with 5 seconds of leader heartbeat timeout , a Raft leader keeps its leadership role as long as at least 1 follower has sent an Append Entries RPC response in the last 5 seconds. Otherwise, the leader Raft node demotes itself to the follower role and fails pending (i.e., locally appended but not yet committed) operations with IndeterminateStateException . This behaviour is due to the asynchronous nature of distributed systems. When the leader cannot get Append Entries RPC responses from some of its followers, it may not accurately decide if those followers are actually crashed, or just temporarily unreachable. If those unresponsive followers are actually alive and can form the majority, they can also elect a new leader among themselves and commit operations replicated by the previous leader. Hence, MicroRaft takes a defensive approach here and makes a leader Raft node step down from the leadership role. It is up to the client to retry an operation which is notified with IndeterminateStateException , because a retry could cause the operation to be committed twice. MicroRaft goes for simplicity and does not employ deduplication (I have plans to implement an opt-in deduplication mechanism in future). If deduplication is needed, it can be done inside StateMachine implementations for now. We will see another code sample to demonstrate how to restore from majority failure. In this part we use the InMemoryRaftStore utility we used in RestoreCrashedRaftNodeTest above. We start a 3-node Raft group, commit an operation, and terminate both of our 2 followers. Then, we try to replicate a new operation. However, in a few seconds the leader will notice that it has not received Append Entries RPC responses from the majority and step down from the leadership role. Because of that, it will also fail our operation with IndeterminateOperationStateException . Since it is a follower now, it will directly reject new RaftNode.replicate() calls with NotLeaderException . At this point, our Raft group is unavailable for RaftNode.replicate() calls, and RaftNode.query() calls for QueryPolicy.LINEARIZABLE , QueryPolicy.LEADER_LEASE , and QueryPolicy.BOUNDED_STALENESS but we can still perform a local query with QueryPolicy.EVENTUAL_CONSISTENCY . If we want to make the Raft group available again, we don't need to restore all crashed Raft nodes. In this particular scenario, it is sufficient to restore only 1 Raft node so that we will have the majority alive again. It is what we do in the last part of the test. Once we have 2 Raft nodes running again, they will be able to elect a new leader. In this example, we waited until the leader demotes itself to the follower role before restarting the crashed Raft nodes. This is not a requirement for restoring crashed Raft nodes, and we did it here only for the sake of example. We can restore a crashed Raft node anytime and if the leader Raft node is still running there may not be a new leader election round and the restarted Raft node could just discover the leader Raft node. Our Raft group will restore its availability as long as there is a leader Raft node taking to the majority (including itself). To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.MajorityFailureTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Please note that you need to have a persistence-layer (i.e., RaftStore implementation) to make this recovery option work. If a crashed Raft node is restarted with the same identity but an empty state, it turns into a Byzantine-failure scenario, where already committed operations can be lost and consistency of the system can be broken. Please see the Corruption or loss of persistent Raft state part for more details. 5. Network partitions Behaviour of a Raft group during a network partition depends on how Raft nodes are divided to different sides of the network partition, and with which Raft nodes our clients are interacting with. If any subset of the Raft nodes manage to form the majority, they remain available. If the Raft leader falls into the minority side, the Raft nodes in the majority side elect a new leader and restore their availability. If our clients cannot talk to the majority side, it means that the Raft group is unavailable from the perspective of the clients. Similar to the majority failure case described in the previous part, if the leader Raft node falls into a minority side of the network partition, it demotes itself to the follower role after the leader heartbeat timeout elapses, and fails all pending operations with IndeterminateStateException . To reiterate, this exception means that the demoted leader cannot decide if those operations have been committed or not. When the network problem is resolved, Raft nodes connect to each other again. The Raft nodes that was on the minority side of the network partition catch up with the other Raft nodes, and the Raft group continues its normal operation. One of the key points of the Raft consensus algorithm's and hence MicroRaft's network partition behaviour is the absence of split-brain . In any network partition scenario, there can be at most one functional leader. We will see how our Raft nodes behave in a network partitioning scenario in the following test. Again, we have a 3-node Raft group here, and we create an artificial network disconnection between the leader and the followers. Since the leader cannot talk to the majority anymore, after the leader heartbeat timeout duration elapses, our leader demotes to the follower role. The followers on the other side elect a new leader among themselves and even commit a new operation. Once we fix the network problem, we see that the old leader connects back to the other Raft nodes, discovers the new leader and gets the new committed operation. Phew! To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.NetworkPartitionTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . 6. Corruption or loss of persistent Raft state If a RestoredRaftState object is created with corrupted or partially-restored Raft state, the safety guarantees of the Raft consensus algorithm no longer hold. For instance, if a flushed log entry is not present in the RestoredRaftState object, then the restored RaftNode may not have a committed operation. If that Raft node becomes leader, it may commit another operation for the same log index with the lost operation and breaks the safety property of the Raft consensus algorithm. RaftStore documents all the durability and integrity guarantees required by its implementations. Hence, it is the responsibility of RaftStore implementations to ensure durability and integrity of the persisted Raft state. RaftNode does not perform any error checks when they are restored with RestoredRaftState objects.","title":"Resiliency and Fault Tolerance"},{"location":"docs/resiliency-and-fault-tolerance/#resiliency-and-fault-tolerance","text":"In this section, we will walk through different types of failure scenarios and discuss how MicroRaft handles each one of them. We will use MicroRaft's local testing utilities to demonstrate those failure scenarios. These utilities are mainly used for testing MicroRaft to a great extent without a distributed setting. Here, we will use them to run a Raft group in a single JVM process and inject different types of failures into the system. In terms of safety, the fundamental guarantee of the Raft consensus algorithm and hence MicroRaft is, operations are committed in a single global order, and a committed operation is never lost, as long as there is no Byzantine failure in the system. In MicroRaft, restarting a Raft node that has no persistence layer with the same identity or restarting it with a corrupted persistence state are examples of Byzantine failure. The availability of a Raft group mainly depends on if the majority (i.e., more than half) of the Raft nodes are alive and able to communicate with each other. The main rule is, 2f + 1 Raft nodes tolerate failure of f Raft nodes. For instance, a 3-node Raft group can tolerate failure of 1 Raft node, or a 5-node Raft group can tolerate failure of 2 Raft nodes without losing availability.","title":"Resiliency and Fault Tolerance"},{"location":"docs/resiliency-and-fault-tolerance/#1-handling-high-system-load","text":"Even if the majority of a Raft group is alive, we may encounter unavailability issues if the Raft group is under high load and cannot keep up with the request rate. In this case, the leader Raft node temporarily stops accepting new requests and notifies the futures returned from the RaftNode methods with CannotReplicateException . This exception means that there are too many operations pending to be committed in the leader's local Raft log, or too many queries pending to be executed, so it temporarily rejects accepting new requests. Clients should apply some backoff before retrying their requests. We will demonstrate this scenario in a test below with a 3-node Raft group. In MicroRaft, a leader does not replicate log entries one by one. Instead, it keeps a buffer for incoming requests and replicates the log entries to the followers in batches in order to improve the throughput. Once this buffer is filled up, the leader stops accepting new requests. In this test, we allow the pending log entries buffer to keep at most 10 requests. We also slow down our followers synthetically by making them sleep for 3 seconds. Then, we start sending requests to the leader. After some time, our requests fail with CannotReplicateException . To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.HighLoadTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository .","title":"1. Handling high system load"},{"location":"docs/resiliency-and-fault-tolerance/#2-minority-failure","text":"Failure of the minority (i.e, less than half) may cause the Raft group to lose availability temporarily, but eventually the Raft group continues to accept and commit new requests. If we have a persistence implementation (i.e, RaftStore ), we can recover failed Raft nodes. On the other hand, if we don't have persistence or cannot recover the persisted Raft data, we can remove failed Raft nodes from the Raft group. Please note that when we remove a Raft node from a Raft group, the majority quorum size is re-calculated based on the new size of the Raft group. In order to replace a non-recoverable Raft node without hurting the overall availability of the Raft group, we should remove the crashed Raft node first and then add a fresh-new one. If Raft nodes are created without an actual RaftStore implementation in the beginning, restarting crashed Raft nodes with the same Raft endpoint identity breaks the safety of the Raft consensus algorithm. Therefore, when there is no persistence layer, the only recovery option for a failed Raft node is to remove it from the Raft group, which is possible only if the majority of the Raft group is up and running. To restart a crashed or terminated Raft node, we can read its persisted state into a RestoredRaftState object. Then, we can use this object to restore the Raft node back. Please note that terminating a Raft node manually without a persistence layer implementation is equivalent to a crash since there is no way to restore the Raft node back with its Raft state. MicroRaft provides a basic in-memory RaftStore implementation to enable crash-recovery testing. In the following code sample, we use this utility, i.e., InMemoryRaftStore , to demonstrate how to recover from Raft node failures. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.RestoreCrashedRaftNodeTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . This time we provide a factory object to enable LocalRaftGroup to create InMemoryRaftStore objects while configuring our Raft nodes. Hence, after we terminate our Raft nodes, we will be able to read their persisted state. Once we start the Raft group, we commit a value via the leader, observe that value with a local query on a follower, and crash a follower. Then, we read its persisted state via our InMemoryRaftStore object and restore the follower back. Please ignore the details of RaftTestUtils.getRestoredState() and RaftTestUtils.getRaftStore() . Once the follower starts running again, it talks to the other Raft nodes, discovers the current leader Raft node and its commit index, and replays all committed operations on its state machine. Our sysout lines in this test print the following: replicate result: value, commit index: 1 monotonic local query successful on follower. query result: value, commit index: 1 monotonic local query successful on restarted follower. query result: value, commit index: 1 When a Raft node starts with a restored Raft state, it discovers the current commit index and replays the Raft log, i.e., automatically applies all the log entries up to the commit index. We should be careful about operations that have side effects because the Raft log replay process triggers those side effects again. Please refer to the State Machine for more details.","title":"2. Minority failure"},{"location":"docs/resiliency-and-fault-tolerance/#3-raft-leader-failure","text":"When a leader Raft node fails, the Raft group temporarily loses availability until the other Raft nodes notice the failure and elect a new leader. Delay of the detection of the leader's failure depends on the leader heartbeat timeout configuration. Please refer to the Configuration section to learn more about the leader election timeout and leader heartbeat timeout configuration parameters. If a client notices that the current leader is not responding, it can contact other Raft nodes in the Raft group in a round-robin fashion and query the leader via the RaftNode.getReport() API. If the leader actually crashes, the followers eventually notice its failure and elect a new leader. Then, our client will be able to discover the new leader Raft endpoint via this API. However, if a client cannot communicate with an alive leader because of an environmental issue, such as a network problem, it cannot replicate new operations, or run QueryPolicy.LINEARIZABLE and QueryPolicy.LEADER_LOCAL queries. It means that the Raft group is unavailable for this particular client. This is due to MicroRaft's simplicity-oriented design philosophy. In MicroRaft, when a follower Raft node receives an API call that requires the leadership role, it does not internally forward the call to the leader Raft node. Instead, it fails the call with NotLeaderException . Please note that this mechanism can be also used for leader discovery. When a client needs to discover the leader, it can try talking to any Raft node. If its call fails with NotLeaderException , the client can check if the exception points the current leader Raft endpoint via NotLeaderException.getLeader() . Otherwise, it can try the same with another Raft node. If a Raft leader crashes before a client receives response for an operation passed to RaftNode.replicate() , there are multiple possibilities: If the leader failed before replicating the operation to any follower, then the operation certainly won't be committed. If the failed leader replicated the operation to at least one follower, then the operation might be committed if a follower having that operation becomes leader. However, another follower could become the new leader and overwrite that operation if it was not replicated to the majority by the crashed leader. The good thing about queries is, they are idempotent. Clients can safely retry their queries on the new leader. It is up to the client to retry an operation whose result is not received, because a retry could cause the operation to be committed twice based on the actual failure scenario. MicroRaft goes for simplicity and does not employ deduplication (I have plans to implement an opt-in deduplication mechanism in future). If deduplication is needed, it can be done inside StateMachine implementations for now. We will see the second scenario described above in a code sample. In the following test, we replicate an operation via the Raft leader, but block the responses sent back from the followers. Even though the leader managed to replicate our operation to the majority, it is not able to commit it because it couldn't learn that the followers also appended this operation to their logs. At this step, we crash the leader. We won't get any response for our operation now since the leader is gone, so we will just re-replicate it with the new leader. The thing is, the previous leader managed to replicate our first operation to the majority, so the new leader will commit it. Since we replicate it for the second time with the new leader, we cause a duplicate commit. When we query the new leader, we see that there are 2 values applied to the state machine. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.RaftLeaderFailureTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Another trick could be designing our operations in an idempotent way and retry them automatically on leader failures, because duplicate commits do not make any harm for idempotent operations. However, it is not very easy to make every type of operation idempotent.","title":"3. Raft leader failure"},{"location":"docs/resiliency-and-fault-tolerance/#4-majority-failure","text":"Failure of the majority causes the Raft group to lose its availability and stop handling new requests. The only recovery option is to recover some of failed Raft nodes so that the majority becomes available again. Otherwise, the Raft group cannot be recovered. MicroRaft does not support any unsafe recovery policy for now. The duration of unavailability depends on how long the majority Raft nodes remain crashed. Clients won't be able to replicate any new operations or run linearizable queries in the meantime. However, we can still run local queries because QueryPolicy.EVENTUAL_CONSISTENCY does not require availability of the majority. In MicroRaft, on each heartbeat tick a leader Raft node checks if it is still in charge, i.e, it has received Append Entries RPC responses from majority quorum size - 1 (majority quorum size minus the leader itself) in the last leader heartbeat timeout period. For instance, in a 3-node Raft group with 5 seconds of leader heartbeat timeout , a Raft leader keeps its leadership role as long as at least 1 follower has sent an Append Entries RPC response in the last 5 seconds. Otherwise, the leader Raft node demotes itself to the follower role and fails pending (i.e., locally appended but not yet committed) operations with IndeterminateStateException . This behaviour is due to the asynchronous nature of distributed systems. When the leader cannot get Append Entries RPC responses from some of its followers, it may not accurately decide if those followers are actually crashed, or just temporarily unreachable. If those unresponsive followers are actually alive and can form the majority, they can also elect a new leader among themselves and commit operations replicated by the previous leader. Hence, MicroRaft takes a defensive approach here and makes a leader Raft node step down from the leadership role. It is up to the client to retry an operation which is notified with IndeterminateStateException , because a retry could cause the operation to be committed twice. MicroRaft goes for simplicity and does not employ deduplication (I have plans to implement an opt-in deduplication mechanism in future). If deduplication is needed, it can be done inside StateMachine implementations for now. We will see another code sample to demonstrate how to restore from majority failure. In this part we use the InMemoryRaftStore utility we used in RestoreCrashedRaftNodeTest above. We start a 3-node Raft group, commit an operation, and terminate both of our 2 followers. Then, we try to replicate a new operation. However, in a few seconds the leader will notice that it has not received Append Entries RPC responses from the majority and step down from the leadership role. Because of that, it will also fail our operation with IndeterminateOperationStateException . Since it is a follower now, it will directly reject new RaftNode.replicate() calls with NotLeaderException . At this point, our Raft group is unavailable for RaftNode.replicate() calls, and RaftNode.query() calls for QueryPolicy.LINEARIZABLE , QueryPolicy.LEADER_LEASE , and QueryPolicy.BOUNDED_STALENESS but we can still perform a local query with QueryPolicy.EVENTUAL_CONSISTENCY . If we want to make the Raft group available again, we don't need to restore all crashed Raft nodes. In this particular scenario, it is sufficient to restore only 1 Raft node so that we will have the majority alive again. It is what we do in the last part of the test. Once we have 2 Raft nodes running again, they will be able to elect a new leader. In this example, we waited until the leader demotes itself to the follower role before restarting the crashed Raft nodes. This is not a requirement for restoring crashed Raft nodes, and we did it here only for the sake of example. We can restore a crashed Raft node anytime and if the leader Raft node is still running there may not be a new leader election round and the restarted Raft node could just discover the leader Raft node. Our Raft group will restore its availability as long as there is a leader Raft node taking to the majority (including itself). To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.MajorityFailureTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Please note that you need to have a persistence-layer (i.e., RaftStore implementation) to make this recovery option work. If a crashed Raft node is restarted with the same identity but an empty state, it turns into a Byzantine-failure scenario, where already committed operations can be lost and consistency of the system can be broken. Please see the Corruption or loss of persistent Raft state part for more details.","title":"4. Majority failure"},{"location":"docs/resiliency-and-fault-tolerance/#5-network-partitions","text":"Behaviour of a Raft group during a network partition depends on how Raft nodes are divided to different sides of the network partition, and with which Raft nodes our clients are interacting with. If any subset of the Raft nodes manage to form the majority, they remain available. If the Raft leader falls into the minority side, the Raft nodes in the majority side elect a new leader and restore their availability. If our clients cannot talk to the majority side, it means that the Raft group is unavailable from the perspective of the clients. Similar to the majority failure case described in the previous part, if the leader Raft node falls into a minority side of the network partition, it demotes itself to the follower role after the leader heartbeat timeout elapses, and fails all pending operations with IndeterminateStateException . To reiterate, this exception means that the demoted leader cannot decide if those operations have been committed or not. When the network problem is resolved, Raft nodes connect to each other again. The Raft nodes that was on the minority side of the network partition catch up with the other Raft nodes, and the Raft group continues its normal operation. One of the key points of the Raft consensus algorithm's and hence MicroRaft's network partition behaviour is the absence of split-brain . In any network partition scenario, there can be at most one functional leader. We will see how our Raft nodes behave in a network partitioning scenario in the following test. Again, we have a 3-node Raft group here, and we create an artificial network disconnection between the leader and the followers. Since the leader cannot talk to the majority anymore, after the leader heartbeat timeout duration elapses, our leader demotes to the follower role. The followers on the other side elect a new leader among themselves and even commit a new operation. Once we fix the network problem, we see that the old leader connects back to the other Raft nodes, discovers the new leader and gets the new committed operation. Phew! To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.faulttolerance.NetworkPartitionTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository .","title":"5. Network partitions"},{"location":"docs/resiliency-and-fault-tolerance/#6-corruption-or-loss-of-persistent-raft-state","text":"If a RestoredRaftState object is created with corrupted or partially-restored Raft state, the safety guarantees of the Raft consensus algorithm no longer hold. For instance, if a flushed log entry is not present in the RestoredRaftState object, then the restored RaftNode may not have a committed operation. If that Raft node becomes leader, it may commit another operation for the same log index with the lost operation and breaks the safety property of the Raft consensus algorithm. RaftStore documents all the durability and integrity guarantees required by its implementations. Hence, it is the responsibility of RaftStore implementations to ensure durability and integrity of the persisted Raft state. RaftNode does not perform any error checks when they are restored with RestoredRaftState objects.","title":"6. Corruption or loss of persistent Raft state"},{"location":"docs/roadmap/","text":"Roadmap I am planning to work on the following tasks in the future, but have no strict plans about their timeline. If you have ideas, just chime in ! Opt-in deduplication mechanism via implementation of the Implementing Linearizability at Large Scale and Low Latency paper. Currently, one can implement deduplication inside his custom StateMachine implementation. I would like to offer a generic and opt-in solution by MicroRaft. Witness replicas possibly via implementation of the Pirogue, a lighter dynamic version of the Raft distributed consensus algorithm paper. Witness replicas participate in quorum calculations but do not keep any state for StateMachine to reduce the storage overhead. When a follower fails, a witness replica can be promoted to the follower role to increase the number of StateMachine replicas. Offload more work from leader to followers. One candidate is transfer of committed log entries. Just like parallel snapshot chunk transfer from followers, a slow follower can get committed log entries from followers. Improve the log replication design. The current log replication design is quite solid but there is still room for improvement. One idea is, once a follower installs a snapshot, the leader can boost that follower by increasing its Append Entries RPC batch size, so that it catches up with the majority faster. Another thing to try is, currently when a leader sends an Append Entries RPC to a follower, it does not send another RPC to that follower either until the follower sends a response, or the Append Entries RPC backoff timeout elapses. During this duration, the leader might append new log entries in its local log. During the Append Entries RPC backoff is enabled for a follower, if more log entries are appended to the leader's log, a few of these log entries can be also sent to the follower.","title":"Roadmap"},{"location":"docs/roadmap/#roadmap","text":"I am planning to work on the following tasks in the future, but have no strict plans about their timeline. If you have ideas, just chime in ! Opt-in deduplication mechanism via implementation of the Implementing Linearizability at Large Scale and Low Latency paper. Currently, one can implement deduplication inside his custom StateMachine implementation. I would like to offer a generic and opt-in solution by MicroRaft. Witness replicas possibly via implementation of the Pirogue, a lighter dynamic version of the Raft distributed consensus algorithm paper. Witness replicas participate in quorum calculations but do not keep any state for StateMachine to reduce the storage overhead. When a follower fails, a witness replica can be promoted to the follower role to increase the number of StateMachine replicas. Offload more work from leader to followers. One candidate is transfer of committed log entries. Just like parallel snapshot chunk transfer from followers, a slow follower can get committed log entries from followers. Improve the log replication design. The current log replication design is quite solid but there is still room for improvement. One idea is, once a follower installs a snapshot, the leader can boost that follower by increasing its Append Entries RPC batch size, so that it catches up with the majority faster. Another thing to try is, currently when a leader sends an Append Entries RPC to a follower, it does not send another RPC to that follower either until the follower sends a response, or the Append Entries RPC backoff timeout elapses. During this duration, the leader might append new log entries in its local log. During the Append Entries RPC backoff is enabled for a follower, if more log entries are appended to the leader's log, a few of these log entries can be also sent to the follower.","title":"Roadmap"},{"location":"docs/setup/","text":"Setup MicroRaft JARs are available in the standard Maven repositories. If you are using Maven, just add the following lines to the dependencies section in your pom.xml : <dependency> <groupId>io.microraft</groupId> <artifactId>microraft</artifactId> <version>0.1</version> </dependency> If you are using HOCON or YAML files for configuration, the following dependencies provide parsers to configure MicroRaft from HOCON and YAML files: <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-hocon</artifactId> <version>0.1</version> </dependency> <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-yaml</artifactId> <version>0.1</version> </dependency> If you don't have Maven but want to build the project on your machine, mvnw is available in the MicroRaft repository. Just hit the following command on your terminal. gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean package Then you can get the JARs from microraft/target , microraft-hocon/target , and microraft-yaml/target directories. Logging MicroRaft depends on the SLF4J library for logging. Actually it is the only dependency of MicroRaft. Make sure you enable the INFO logging level for the io.microraft package. If you are going hard, you can also give the DEBUG level a shot, but I assure you it will be a bumpy ride. What is next? OK. You have the MicroRaft JAR in your classpath, and the logging also looks good. Now you are ready to build your CP distributed system! Then why don't you start with checking out the main abstractions defined in MicroRaft?","title":"Setup"},{"location":"docs/setup/#setup","text":"MicroRaft JARs are available in the standard Maven repositories. If you are using Maven, just add the following lines to the dependencies section in your pom.xml : <dependency> <groupId>io.microraft</groupId> <artifactId>microraft</artifactId> <version>0.1</version> </dependency> If you are using HOCON or YAML files for configuration, the following dependencies provide parsers to configure MicroRaft from HOCON and YAML files: <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-hocon</artifactId> <version>0.1</version> </dependency> <dependency> <groupId>io.microraft</groupId> <artifactId>microraft-yaml</artifactId> <version>0.1</version> </dependency> If you don't have Maven but want to build the project on your machine, mvnw is available in the MicroRaft repository. Just hit the following command on your terminal. gh repo clone MicroRaft/MicroRaft && cd MicroRaft && ./mvnw clean package Then you can get the JARs from microraft/target , microraft-hocon/target , and microraft-yaml/target directories.","title":"Setup"},{"location":"docs/setup/#logging","text":"MicroRaft depends on the SLF4J library for logging. Actually it is the only dependency of MicroRaft. Make sure you enable the INFO logging level for the io.microraft package. If you are going hard, you can also give the DEBUG level a shot, but I assure you it will be a bumpy ride.","title":"Logging"},{"location":"docs/setup/#what-is-next","text":"OK. You have the MicroRaft JAR in your classpath, and the logging also looks good. Now you are ready to build your CP distributed system! Then why don't you start with checking out the main abstractions defined in MicroRaft?","title":"What is next?"},{"location":"docs/tutorial-building-an-atomic-register/","text":"Tutorial: Building an Atomic Register In this section, we will build an atomic register on top of MicroRaft to demonstrate how to implement and use MicroRaft's main abstractions. Our atomic register will consist of a single value and only 3 operations: set , compare-and-set , and get . In order to keep things simple, we will not run our Raft group in a distributed setting. Instead, will run each Raft node on a different thread and make them communicate with each other via multi-threaded queues. If you haven't read the Main Abstractions section yet, I highly recommend you to read that section before this tutorial. All the code shown here are compiling and available in the MicroRaft Github repository . You can clone the repository and run the code samples on your machine to try each part yourself. I intentionally duplicated a lot of code in the test classes below to put all pieces together so that you can see what is going on without navigating through multiple classes. Let's crank up the engine! 1. Implementing the Main Abstractions We will start with writing our RaftEndpoint , StateMachine and Transport classes. We can use the default implementations of RaftNodeExecutor , RaftModel and RaftModelFactory abstractions. Since all of our Raft nodes will run in the same JVM process, we also don't need any serialization logic inside our Transport implementation. Last, we will also skip persistence. Our Raft nodes will keep their state only in memory. RaftEndpoint First, we need to implement RaftEndpoint to represent identity of our Raft nodes. Since we don't distribute our Raft nodes to multiple servers in this tutorial, we don't really need IP addresses. We can simply identify our Raft nodes with strings IDs and keep a mapping of unique IDs to Raft nodes so that we can deliver RaftMessage objects to target Raft nodes. Let's write a LocalRaftEndpoint class as below. We will generate unique Raft endpoints via its static LocalRaftEndpoint.newEndpoint() method. You can also see this class in the MicroRaft Github repository . StateMachine We will implement our atomic register state machine iteratively. In the first iteration, we will create our Raft nodes and elect a leader without committing any atomic register operation. So the first version of our state machine, which is shown below, does not have any execution and snapshotting logic for our atomic register. We implement StateMachine.getNewTermOperation() in our first version of state machine. This method returns an operation which will be committed every time a new leader is elected. This is actually related to the single-server membership change bug in the Raft consensus algorithm rather than our atomic register logic. Once we see that we are able to form a Raft group and elect a leader, we will extend this class to implement the missing functionality. You can also see this class in the MicroRaft Github repository . Transport We are almost there to run our first test for bootstrapping a Raft group and electing a leader. The only missing piece is Transport . Recall that Transport is responsible for sending Raft messages to other Raft nodes (serialization and networking). Since our Raft nodes will run in a single JVM process in this tutorial, we will skip serialization. To mimic networking, we will keep a mapping of Raft endpoints to Raft nodes on each Transport object and pass Raft messages to between Raft nodes by using this mapping. Our LocalTransport class is shown below. You can also see this class in the MicroRaft Github repository . 2. Bootstrapping the Raft group Now we have all the required functionality to start our Raft group and elect a leader. Let's write our first test. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.LeaderElectionTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Ok. That is a big piece of code, but no worries. We will swallow it one piece at a time. Before bootstrapping a new Raft group, we first decide on its initial member list, i.e., the list of Raft endpoints. The very same initial Raft group member list must be provided to all Raft nodes, including the ones we will start later and join to our already-running Raft group. It is what we do in the beginning of the class by populating initialMembers with 3 unique Raft endpoints. startRaftGroup() calls createRaftNode() for each Raft endpoint and then starts the created Raft nodes. When a new Raft node is created, it does not know how to talk to the other Raft nodes. Therefore, we pass created Raft node objects to enableDiscovery() to enable them to talk to each other. This method just adds a given Raft node to the discovery maps of the LocalTransport objects of the other Raft nodes. Once we create a Raft node via RaftNodeBuilder , its initial status is RaftNodeStatus.INITIAL and it does not execute the Raft consensus algorithm in this status. When RaftNode.start() is called, its status becomes RaftNodeStatus.ACTIVE and the Raft node internally submits a task to its RaftNodeExecutor to check if there is a leader. Since we are starting a new Raft group in this test, obviously there is no leader yet so our Raft nodes will start a new leader election round. The actual test method is rather short. It calls waitUntilLeaderElected() to get the leader Raft node and asserts that a Raft node is returned as leader. Please note that this method offers the discovery functionality to find the leader. It just waits until all Raft nodes report the same Raft node as the leader. Let's run this code and see the logs. The logs start like following: 23:54:38.121 INFO - [RaftNode] node2<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.121 INFO - [RaftNode] node3<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.121 INFO - [RaftNode] node1<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.123 INFO - [RaftNode] node2<default> Status is set to ACTIVE 23:54:38.123 INFO - [RaftNode] node3<default> Status is set to ACTIVE 23:54:38.123 INFO - [RaftNode] node1<default> Status is set to ACTIVE 23:54:38.125 INFO - [RaftNode] node3<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 node2 node3 - FOLLOWER this (ACTIVE) ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node2<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 node2 - FOLLOWER this (ACTIVE) node3 ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node3<default> started. 23:54:38.125 INFO - [RaftNode] node1<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 - FOLLOWER this (ACTIVE) node2 node3 ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node2<default> started. 23:54:38.125 INFO - [RaftNode] node1<default> started. Each RaftNode first prints the id and initial member list of the Raft group. When we call RaftNode.start() , they switch to the RaftNodeStatus.ACTIVE status and print a summary of the Raft group, including the member list. Each node also marks itself in the Raft group member list, and the reason of why the log is printed so that we can follow what is going on. After this part, our Raft nodes start a leader election. Raft can be very chatty during leader elections. For the sake of simplicity, we will just skip the leader election logs and look at the final status. 23:54:39.141 INFO - [VoteResponseHandler] node3<default> Vote granted from node1 for term: 2, number of votes: 2, majority: 2 23:54:39.141 INFO - [VoteResponseHandler] node3<default> We are the LEADER! 23:54:39.149 INFO - [AppendEntriesRequestHandler] node1<default> Setting leader: node3 23:54:39.149 INFO - [RaftNode] node3<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 node2 node3 - LEADER this (ACTIVE) ] reason: ROLE_CHANGE 23:54:39.149 INFO - [AppendEntriesRequestHandler] node2<default> Setting leader: node3 23:54:39.149 INFO - [RaftNode] node1<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 - FOLLOWER this (ACTIVE) node2 node3 - LEADER ] reason: ROLE_CHANGE 23:54:39.149 INFO - [RaftNode] node2<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 node2 - FOLLOWER this (ACTIVE) node3 - LEADER ] reason: ROLE_CHANGE As you see, we managed to elect our leader in the 2nd term. It means that we had a split-vote situation in the 1st term. This is quite normal because in our test code we start our Raft nodes at the same time and each Raft node just votes for itself during the first term. Please keep in mind that in your run, another Raft node could become the leader. 3. Sending requests Now it is time to implement the set , compare-and-set , and get operations we talked before for our atomic register state machine. We must ensure that they are implemented in a deterministic way, because it is a fundamental requirement of the replicated state machines approach. MicroRaft guarantees that each Raft node will execute committed operations in the same order and since our operations run deterministically, we know that our state machines will end up with the same state after they execute committed operations. To implement these operations, we will extend our AtomicRegister class instead of modifying it so that the readers can follow the different stages of this tutorial. The new state machine class is below. We define an interface, AtomicRegisterOperation , as a marker for the operations we will execute on our state machine. There are 3 inner classes implementing this interface for the set , compare-and-set and get operations, and accompanying static methods to create their instances. We will pass an AtomicRegisterOperation object to RaftNode.replicate() and once it is committed by MicroRaft it will be passed to our state machine for execution. Our new state machine class handles these AtomicRegisterOperation objects in the runOperation() method. set updates the atomic register with the given value and returns its previous value, compare-and-set updates the atomic register with the given new value only if its current value is equal to the given current value, and get simply returns the current value of the atomic register. Please note that the snapshotting logic is still missing and will be implemented later in the tutorial. You can also see this class in the MicroRaft Github repository . Committing operations In the next test we will use the new state machine to start our Raft nodes. We will commit a number of operations on the Raft group and verify their results. We replicate 2 set operations, 2 compare-and-set operations, and a get operation at the end. After each operation, we verify that its commit index is greater than the commit index of the previous operation. $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . We use RaftNode.replicate() to replicate and commit operations on the Raft group. Most of the Raft node APIs, including RaftNode.replicate() , return CompletableFuture<Ordered> objects. For RaftNode.replicate() , Ordered provides return value of the executed operation and on which Raft log index the operation has been committed. The output of the first 2 sysout lines are below. Please note that set returns previous value of the atomic register. 1st operation commit index: 2, result: null 2nd operation commit index: 3, result: value1 Then we commit 2 cas operations. The first cas manages to update the atomic register, but the second one fails because the atomic register value is different from the expected value. 3rd operation commit index: 4, result: true 4th operation commit index: 5, result: false The last operation is a get to read the current value of the atomic register. 5th operation commit index: 6, result: value3 If we call RaftNode.replicate() on a follower or candidate Raft node, the returned CompletableFuture<Ordered> object is simply notified with NotLeaderException , which also provides Raft endpoint of the leader Raft node. I am not going to build an advanced RPC system in front of MicroRaft here, but when we use MicroRaft in a distributed setting, we can build a retry mechanism in the RPC layer to forward a failed operation to the Raft endpoint given in the exception. NotLeaderException may not specify any leader as well, for example if there is an ongoing leader election round, or the Raft node we contacted does not know the leader yet. In this case, our RPC layer could retry the operation on each Raft node in a round robin fashion until it discovers the new leader. Performing queries The last operation we committed in our previous test is get , which is actually a query (i.e, read only) operation. Even though it does not mutate the state machine, since we used RaftNode.replicate() , it is appended to the Raft log and committed similar to the other mutating operations. If we had persistence, it means this approach would increase our disk usage unnecessarily because we will persist every new entry in the Raft log, even if it is a query. Actually, this is a sub-optimal approach. MicroRaft offers a separate API, RaftNode.query() , to handle queries more efficiently. There are 3 policies for queries , each with a different consistency guarantee: QueryPolicy.LINEARIZABLE : We can perform a linearizable query with this policy. MicroRaft employs the optimization described in \u00a7 6.4: Processing read-only queries more efficiently of the Raft dissertation to preserve linearizability without growing the internal Raft log. We need to hit the leader Raft node to execute a linearizable query. QueryPolicy.LEADER_LEASE : We can run a query locally on the leader Raft node without talking to the majority. If the called Raft node is not the leader, the returned CompletableFuture<Ordered> object is notified with NotLeaderException . QueryPolicy.BOUNDED_STALENESS We can run a query locally on a follower or a learner Raft node if it has received an AppendEntries RPC from the leader in the last leader heartbeat timeout duration. QueryPolicy.EVENTUAL_CONSISTENCY : We can use this policy to run a query locally on any Raft node independent of its current role. This policy provides the weakest consistency guarantee, but it can help us to distribute our query-workload by utilizing followers. We can also utilize Ordered to preserve a monotonic view of the Raft group state. MicroRaft also employs leader stickiness and auto-demotion of leaders on loss of majority heartbeats. Leader stickiness means that a follower does not vote for another Raft node before the leader heartbeat timeout elapses after the last received Append Entries or Install Snapshot RPC. Dually, a leader Raft node automatically demotes itself to the follower role if it does not receive Append Entries RPC responses from the majority during the leader heartbeat timeout duration. Along with these techniques, QueryPolicy.LEADER_LEASE can be used for performing linearizable queries without talking to the majority if clock drifts and network delays are bounded. However, bounding clock drifts and network delays is not an easy job. Hence, QueryPolicy.LEADER_LEASE may cause reading stale data if a Raft node still considers itself as the leader because of a clock drift, while the other Raft nodes have elected a new leader and committed new operations. Moreover, QueryPolicy.LEADER_LEASE and QueryPolicy.LINEARIZABLE have the same processing cost since only the leader Raft node runs a given query for both policies. QueryPolicy.LINEARIZABLE guarantees linearizability with an extra RTT latency overhead compared to QueryPolicy.LEADER_LEASE . For these reasons, QueryPolicy.LINEARIZABLE is the recommended policy for linearizable queries, and QueryPolicy.LEADER_LEASE should be used carefully. Linearizable queries Ok. Let's write another test to use the query API for get . We will first set a value to the atomic register and then get the value back with a linearizable query. Just ignore the third parameter passed to the RaftNode.query() call for now. We will talk about it in a minute. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.LinearizableQueryTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . The output of the sysout lines are below: set operation commit index: 2 get operation commit index: 2, result: value As we discussed above, MicroRaft handles linearizable queries without appending a new entry to the Raft log. So our query is executed at the last committed log index. That is why both commit indices are the same in the output. Monotonic local queries QueryPolicy.LEADER_LEASE , QueryPolicy.BOUNDED_STALENESS and QueryPolicy.EVENTUAL_CONSISTENCY can be easily used if monotonicity is sufficient for query results. This is where Ordered comes in handy. A client can track commit indices observed via returned Ordered objects and use the greatest observed commit index to preserve monotonicity while issuing a local query to a Raft node. If the local commit index of a Raft node is smaller than the commit index passed to the RaftNode.query() call, the returned CompletableFuture object fails with LaggingCommitIndexException . This exception means that the state observed by the client is more up-to-date than the contacted Raft node's state. In this case, the client can retry its query on another Raft node. Please refer to \u00a7 6.4.1 of the Raft dissertation for more details. We will make a little trick to demonstrate how to maintain the monotonicity of the observed Raft group state for the local query policies . Recall that LocalTransport keeps a discovery map to send Raft messages to target Raft nodes. When we create a new Raft node, we add it to the other Raft nodes' discovery maps . This time, we will do exactly the reverse to block the communication between the leader and the follower. Let's add the following method to our LocalTransport class: public void undiscoverNode(RaftNode node) { RaftEndpoint endpoint = node.getLocalEndpoint(); if (localEndpoint.equals(endpoint)) { throw new IllegalArgumentException(localEndpoint + \" cannot undiscover itself!\"); } nodes.remove(node.getLocalEndpoint(), node); } We use this method in our new test below. We block the communication between the leader and follower after we set a value to the atomic register. Then we set another value which will not be replicated to the disconnected follower. After this step, we issue a query on the leader. We are also tracking the commit index we observed. Now, we switch to the disconnected follower and issue a new local query by passing the last observed commit index. Since the disconnected follower does not have the second commit, it cannot satisfy the monotonicity we demand, hence our query fails with LaggingCommitIndexException . To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.MonotonicLocalQueryTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . 4. Snapshotting Now it is time to implement the missing snapshotting functionality in our atomic register state machine. Let's talk about snapshotting first. The Raft log grows during the lifecycle of the Raft group as more operations are committed. However, in a real-world system we cannot allow it to grow unboundedly. As the Raft log grows longer, it will consume more space both in memory and disk, and cause a lagging follower to catch up with the majority in a longer duration. Raft solves this problem by taking a snapshot of the state machine at current commit index and discarding all log entries up to it. MicroRaft implements snapshotting by putting an upper bound on the number of log entries kept in Raft log. It takes a snapshot of the state machine at every N commits and shrinks the log. N is configurable via RaftConfig.setCommitCountToTakeSnapshot() . A snapshot is represented as a list of chunks, where a chunk can be any object provided by the state machine. StateMachine contains 2 methods for the snapshotting functionality: takeSnapshot() and installSnapshot() . Similar to what we did in the previous part, we will extend OperableAtomicRegister to implement these methods. Since our atomic register state machine consists of a single value, we just create a single snapshot chunk object in takeSnapshot() . Dually, to install a snapshot, we overwrite the value of the atomic register with the value present in the received snapshot chunk object. MicroRaft guarantees that commit index of an installed snapshot is always greater than the last commit index observed by the state machine. You can also see this class in the MicroRaft Github repository . We have the following test to demonstrate how snapshotting works in MicroRaft. In createRaftNode() , we configure our Raft nodes to take a new snapshot at every 100 commits. Similar to what we did in the previous test, we block the communication between the leader and a follower, and fill up the leader's Raft log with new commits until it takes a snapshot. When we allow the leader to communicate with the follower again, the follower catches up with the leader by transferring the snapshot. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.SnapshotInstallationTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . 5. Extending the Raft group Ok. We covered a lot of topics up until this part. There are only a few things left to discuss. In this part, we will see how we can perform changes in Raft group member lists. Changing the Raft group member list MicroRaft supports membership changes in Raft groups via RaftNode.changeMembership() . Let's first see the rules to realize membership changes in Raft groups. Raft group membership changes are appended to the internal Raft log as regular log entries and committed similar to user-supplied operations. Therefore, Raft group membership changes require the majority of the Raft group to be operational. When a membership change is committed, its commit index is used to denote the new member list of the Raft group and called group members commit index . Relatedly, when a membership change is triggered via RaftNode.changeMembership() , the current group members commit index must be provided. Last, MicroRaft allows one membership change at a time in a Raft group and more complex changes must be applied as a series of single changes. Since we know the rules for member list changes now, let's see some code. In our last test, we want to improve our 3-member Raft group's degree of fault tolerance by adding 2 new members (majority quorum size of 3 = 2 -> majority quorum size of 5 = 3). To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.ChangeRaftGroupMemberListTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . In this test, we create a new Raft endpoint, endpoint4 , and add it to the Raft group in the following lines: RaftEndpoint endpoint4 = LocalRaftEndpoint.newEndpoint(); // group members commit index of the initial Raft group members is 0. RaftGroupMembers newMemberList1 = leader.changeMembership(endpoint4, MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER, 0).join().getResult(); System.out.println(\"New member list: \" + newMemberList1.getMembers() + \", majority: \" + newMemberList1.getMajorityQuorumSize() + \", commit index: \" + newMemberList1.getLogIndex()); // endpoint4 is now part of the member list. Let's start its Raft node RaftNode raftNode4 = createRaftNode(endpoint4); raftNode4.start(); Please notice that we pass 0 for the third argument to RaftNode.changeMembership() , because our Raft group operates with the initial member list whose group members commit index is 0 . After this call returns, endpoint4 is part of the Raft group, so we start its Raft node as well. Our sysout line here prints the following: New member list: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}, LocalRaftEndpoint{id=node4}], majority: 3, commit index: 3 As you see, our Raft group has 4 members now, whose majority quorum size is 3. Actually, running a 4-node Raft group has no advantage over running a 3-node Raft group in terms of the degree of availability because both of them can handle failure of only 1 Raft node. Since we want to achieve better availability, we will add one more Raft node to our Raft group. So we create another Raft endpoint, endpoint5 , add it to the Raft group, and start its Raft node. However, as the group members commit index parameter, we pass newMemberList1.getLogIndex() which is the log index endpoint4 is added to the Raft group. Please note that we could also get the current Raft group members commit index via RaftNode.getCommittedMembers() . Our second sysout line prints the following: New member list: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}, LocalRaftEndpoint{id=node4}, LocalRaftEndpoint{id=node5}], majority: 3, commit index: 5 Now we have 5 nodes in our Raft group with the majority quorum size of 3. It means that now our Raft group can tolerate failure of 2 Raft nodes and still remain operational. Voila! In this example, for the sake of simplicity, we add new Raft nodes to the Raft group with MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER . With this mode, the Raft node is directly added as a follower , and the majority quorum size of the Raft group increases to 3. However, in real life use cases, Raft nodes can contain large state, and it may take some time until the new Raft node catches up with the leader. Because of this, increasing the majority quorum size may cause availability gaps if failures occur before the new Raft node catches up. To prevent this, MicroRaft offers another membership change mode, MembershipChangeMode.ADD_LEARNER , to add new Raft nodes. With this mode, the new Raft node is added with the learner role. Learner Raft nodes are excluded in majority calculations, hence adding a new learner Raft node to the Raft group does not change the majority quorum size. Once the new learner Raft node catches up, it can be promoted to the follower role by triggering another membership change: MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER . What's next? In the next section, we will see how MicroRaft deals with failures. Just keep calm and carry on !","title":"Tutorial: Building an Atomic Register"},{"location":"docs/tutorial-building-an-atomic-register/#tutorial-building-an-atomic-register","text":"In this section, we will build an atomic register on top of MicroRaft to demonstrate how to implement and use MicroRaft's main abstractions. Our atomic register will consist of a single value and only 3 operations: set , compare-and-set , and get . In order to keep things simple, we will not run our Raft group in a distributed setting. Instead, will run each Raft node on a different thread and make them communicate with each other via multi-threaded queues. If you haven't read the Main Abstractions section yet, I highly recommend you to read that section before this tutorial. All the code shown here are compiling and available in the MicroRaft Github repository . You can clone the repository and run the code samples on your machine to try each part yourself. I intentionally duplicated a lot of code in the test classes below to put all pieces together so that you can see what is going on without navigating through multiple classes. Let's crank up the engine!","title":"Tutorial: Building an Atomic Register"},{"location":"docs/tutorial-building-an-atomic-register/#1-implementing-the-main-abstractions","text":"We will start with writing our RaftEndpoint , StateMachine and Transport classes. We can use the default implementations of RaftNodeExecutor , RaftModel and RaftModelFactory abstractions. Since all of our Raft nodes will run in the same JVM process, we also don't need any serialization logic inside our Transport implementation. Last, we will also skip persistence. Our Raft nodes will keep their state only in memory.","title":"1. Implementing the Main Abstractions"},{"location":"docs/tutorial-building-an-atomic-register/#raftendpoint","text":"First, we need to implement RaftEndpoint to represent identity of our Raft nodes. Since we don't distribute our Raft nodes to multiple servers in this tutorial, we don't really need IP addresses. We can simply identify our Raft nodes with strings IDs and keep a mapping of unique IDs to Raft nodes so that we can deliver RaftMessage objects to target Raft nodes. Let's write a LocalRaftEndpoint class as below. We will generate unique Raft endpoints via its static LocalRaftEndpoint.newEndpoint() method. You can also see this class in the MicroRaft Github repository .","title":"RaftEndpoint"},{"location":"docs/tutorial-building-an-atomic-register/#statemachine","text":"We will implement our atomic register state machine iteratively. In the first iteration, we will create our Raft nodes and elect a leader without committing any atomic register operation. So the first version of our state machine, which is shown below, does not have any execution and snapshotting logic for our atomic register. We implement StateMachine.getNewTermOperation() in our first version of state machine. This method returns an operation which will be committed every time a new leader is elected. This is actually related to the single-server membership change bug in the Raft consensus algorithm rather than our atomic register logic. Once we see that we are able to form a Raft group and elect a leader, we will extend this class to implement the missing functionality. You can also see this class in the MicroRaft Github repository .","title":"StateMachine"},{"location":"docs/tutorial-building-an-atomic-register/#transport","text":"We are almost there to run our first test for bootstrapping a Raft group and electing a leader. The only missing piece is Transport . Recall that Transport is responsible for sending Raft messages to other Raft nodes (serialization and networking). Since our Raft nodes will run in a single JVM process in this tutorial, we will skip serialization. To mimic networking, we will keep a mapping of Raft endpoints to Raft nodes on each Transport object and pass Raft messages to between Raft nodes by using this mapping. Our LocalTransport class is shown below. You can also see this class in the MicroRaft Github repository .","title":"Transport"},{"location":"docs/tutorial-building-an-atomic-register/#2-bootstrapping-the-raft-group","text":"Now we have all the required functionality to start our Raft group and elect a leader. Let's write our first test. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.LeaderElectionTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . Ok. That is a big piece of code, but no worries. We will swallow it one piece at a time. Before bootstrapping a new Raft group, we first decide on its initial member list, i.e., the list of Raft endpoints. The very same initial Raft group member list must be provided to all Raft nodes, including the ones we will start later and join to our already-running Raft group. It is what we do in the beginning of the class by populating initialMembers with 3 unique Raft endpoints. startRaftGroup() calls createRaftNode() for each Raft endpoint and then starts the created Raft nodes. When a new Raft node is created, it does not know how to talk to the other Raft nodes. Therefore, we pass created Raft node objects to enableDiscovery() to enable them to talk to each other. This method just adds a given Raft node to the discovery maps of the LocalTransport objects of the other Raft nodes. Once we create a Raft node via RaftNodeBuilder , its initial status is RaftNodeStatus.INITIAL and it does not execute the Raft consensus algorithm in this status. When RaftNode.start() is called, its status becomes RaftNodeStatus.ACTIVE and the Raft node internally submits a task to its RaftNodeExecutor to check if there is a leader. Since we are starting a new Raft group in this test, obviously there is no leader yet so our Raft nodes will start a new leader election round. The actual test method is rather short. It calls waitUntilLeaderElected() to get the leader Raft node and asserts that a Raft node is returned as leader. Please note that this method offers the discovery functionality to find the leader. It just waits until all Raft nodes report the same Raft node as the leader. Let's run this code and see the logs. The logs start like following: 23:54:38.121 INFO - [RaftNode] node2<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.121 INFO - [RaftNode] node3<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.121 INFO - [RaftNode] node1<default> Starting for default with 3 members: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}] 23:54:38.123 INFO - [RaftNode] node2<default> Status is set to ACTIVE 23:54:38.123 INFO - [RaftNode] node3<default> Status is set to ACTIVE 23:54:38.123 INFO - [RaftNode] node1<default> Status is set to ACTIVE 23:54:38.125 INFO - [RaftNode] node3<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 node2 node3 - FOLLOWER this (ACTIVE) ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node2<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 node2 - FOLLOWER this (ACTIVE) node3 ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node3<default> started. 23:54:38.125 INFO - [RaftNode] node1<default> Raft Group Members {groupId: default, size: 3, term: 0, logIndex: 0} [ node1 - FOLLOWER this (ACTIVE) node2 node3 ] reason: STATUS_CHANGE 23:54:38.125 INFO - [RaftNode] node2<default> started. 23:54:38.125 INFO - [RaftNode] node1<default> started. Each RaftNode first prints the id and initial member list of the Raft group. When we call RaftNode.start() , they switch to the RaftNodeStatus.ACTIVE status and print a summary of the Raft group, including the member list. Each node also marks itself in the Raft group member list, and the reason of why the log is printed so that we can follow what is going on. After this part, our Raft nodes start a leader election. Raft can be very chatty during leader elections. For the sake of simplicity, we will just skip the leader election logs and look at the final status. 23:54:39.141 INFO - [VoteResponseHandler] node3<default> Vote granted from node1 for term: 2, number of votes: 2, majority: 2 23:54:39.141 INFO - [VoteResponseHandler] node3<default> We are the LEADER! 23:54:39.149 INFO - [AppendEntriesRequestHandler] node1<default> Setting leader: node3 23:54:39.149 INFO - [RaftNode] node3<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 node2 node3 - LEADER this (ACTIVE) ] reason: ROLE_CHANGE 23:54:39.149 INFO - [AppendEntriesRequestHandler] node2<default> Setting leader: node3 23:54:39.149 INFO - [RaftNode] node1<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 - FOLLOWER this (ACTIVE) node2 node3 - LEADER ] reason: ROLE_CHANGE 23:54:39.149 INFO - [RaftNode] node2<default> Raft Group Members {groupId: default, size: 3, term: 2, logIndex: 0} [ node1 node2 - FOLLOWER this (ACTIVE) node3 - LEADER ] reason: ROLE_CHANGE As you see, we managed to elect our leader in the 2nd term. It means that we had a split-vote situation in the 1st term. This is quite normal because in our test code we start our Raft nodes at the same time and each Raft node just votes for itself during the first term. Please keep in mind that in your run, another Raft node could become the leader.","title":"2. Bootstrapping the Raft group"},{"location":"docs/tutorial-building-an-atomic-register/#3-sending-requests","text":"Now it is time to implement the set , compare-and-set , and get operations we talked before for our atomic register state machine. We must ensure that they are implemented in a deterministic way, because it is a fundamental requirement of the replicated state machines approach. MicroRaft guarantees that each Raft node will execute committed operations in the same order and since our operations run deterministically, we know that our state machines will end up with the same state after they execute committed operations. To implement these operations, we will extend our AtomicRegister class instead of modifying it so that the readers can follow the different stages of this tutorial. The new state machine class is below. We define an interface, AtomicRegisterOperation , as a marker for the operations we will execute on our state machine. There are 3 inner classes implementing this interface for the set , compare-and-set and get operations, and accompanying static methods to create their instances. We will pass an AtomicRegisterOperation object to RaftNode.replicate() and once it is committed by MicroRaft it will be passed to our state machine for execution. Our new state machine class handles these AtomicRegisterOperation objects in the runOperation() method. set updates the atomic register with the given value and returns its previous value, compare-and-set updates the atomic register with the given new value only if its current value is equal to the given current value, and get simply returns the current value of the atomic register. Please note that the snapshotting logic is still missing and will be implemented later in the tutorial. You can also see this class in the MicroRaft Github repository .","title":"3. Sending requests"},{"location":"docs/tutorial-building-an-atomic-register/#committing-operations","text":"In the next test we will use the new state machine to start our Raft nodes. We will commit a number of operations on the Raft group and verify their results. We replicate 2 set operations, 2 compare-and-set operations, and a get operation at the end. After each operation, we verify that its commit index is greater than the commit index of the previous operation. $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.OperationCommitTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . We use RaftNode.replicate() to replicate and commit operations on the Raft group. Most of the Raft node APIs, including RaftNode.replicate() , return CompletableFuture<Ordered> objects. For RaftNode.replicate() , Ordered provides return value of the executed operation and on which Raft log index the operation has been committed. The output of the first 2 sysout lines are below. Please note that set returns previous value of the atomic register. 1st operation commit index: 2, result: null 2nd operation commit index: 3, result: value1 Then we commit 2 cas operations. The first cas manages to update the atomic register, but the second one fails because the atomic register value is different from the expected value. 3rd operation commit index: 4, result: true 4th operation commit index: 5, result: false The last operation is a get to read the current value of the atomic register. 5th operation commit index: 6, result: value3 If we call RaftNode.replicate() on a follower or candidate Raft node, the returned CompletableFuture<Ordered> object is simply notified with NotLeaderException , which also provides Raft endpoint of the leader Raft node. I am not going to build an advanced RPC system in front of MicroRaft here, but when we use MicroRaft in a distributed setting, we can build a retry mechanism in the RPC layer to forward a failed operation to the Raft endpoint given in the exception. NotLeaderException may not specify any leader as well, for example if there is an ongoing leader election round, or the Raft node we contacted does not know the leader yet. In this case, our RPC layer could retry the operation on each Raft node in a round robin fashion until it discovers the new leader.","title":"Committing operations"},{"location":"docs/tutorial-building-an-atomic-register/#performing-queries","text":"The last operation we committed in our previous test is get , which is actually a query (i.e, read only) operation. Even though it does not mutate the state machine, since we used RaftNode.replicate() , it is appended to the Raft log and committed similar to the other mutating operations. If we had persistence, it means this approach would increase our disk usage unnecessarily because we will persist every new entry in the Raft log, even if it is a query. Actually, this is a sub-optimal approach. MicroRaft offers a separate API, RaftNode.query() , to handle queries more efficiently. There are 3 policies for queries , each with a different consistency guarantee: QueryPolicy.LINEARIZABLE : We can perform a linearizable query with this policy. MicroRaft employs the optimization described in \u00a7 6.4: Processing read-only queries more efficiently of the Raft dissertation to preserve linearizability without growing the internal Raft log. We need to hit the leader Raft node to execute a linearizable query. QueryPolicy.LEADER_LEASE : We can run a query locally on the leader Raft node without talking to the majority. If the called Raft node is not the leader, the returned CompletableFuture<Ordered> object is notified with NotLeaderException . QueryPolicy.BOUNDED_STALENESS We can run a query locally on a follower or a learner Raft node if it has received an AppendEntries RPC from the leader in the last leader heartbeat timeout duration. QueryPolicy.EVENTUAL_CONSISTENCY : We can use this policy to run a query locally on any Raft node independent of its current role. This policy provides the weakest consistency guarantee, but it can help us to distribute our query-workload by utilizing followers. We can also utilize Ordered to preserve a monotonic view of the Raft group state. MicroRaft also employs leader stickiness and auto-demotion of leaders on loss of majority heartbeats. Leader stickiness means that a follower does not vote for another Raft node before the leader heartbeat timeout elapses after the last received Append Entries or Install Snapshot RPC. Dually, a leader Raft node automatically demotes itself to the follower role if it does not receive Append Entries RPC responses from the majority during the leader heartbeat timeout duration. Along with these techniques, QueryPolicy.LEADER_LEASE can be used for performing linearizable queries without talking to the majority if clock drifts and network delays are bounded. However, bounding clock drifts and network delays is not an easy job. Hence, QueryPolicy.LEADER_LEASE may cause reading stale data if a Raft node still considers itself as the leader because of a clock drift, while the other Raft nodes have elected a new leader and committed new operations. Moreover, QueryPolicy.LEADER_LEASE and QueryPolicy.LINEARIZABLE have the same processing cost since only the leader Raft node runs a given query for both policies. QueryPolicy.LINEARIZABLE guarantees linearizability with an extra RTT latency overhead compared to QueryPolicy.LEADER_LEASE . For these reasons, QueryPolicy.LINEARIZABLE is the recommended policy for linearizable queries, and QueryPolicy.LEADER_LEASE should be used carefully.","title":"Performing queries"},{"location":"docs/tutorial-building-an-atomic-register/#linearizable-queries","text":"Ok. Let's write another test to use the query API for get . We will first set a value to the atomic register and then get the value back with a linearizable query. Just ignore the third parameter passed to the RaftNode.query() call for now. We will talk about it in a minute. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.LinearizableQueryTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . The output of the sysout lines are below: set operation commit index: 2 get operation commit index: 2, result: value As we discussed above, MicroRaft handles linearizable queries without appending a new entry to the Raft log. So our query is executed at the last committed log index. That is why both commit indices are the same in the output.","title":"Linearizable queries"},{"location":"docs/tutorial-building-an-atomic-register/#monotonic-local-queries","text":"QueryPolicy.LEADER_LEASE , QueryPolicy.BOUNDED_STALENESS and QueryPolicy.EVENTUAL_CONSISTENCY can be easily used if monotonicity is sufficient for query results. This is where Ordered comes in handy. A client can track commit indices observed via returned Ordered objects and use the greatest observed commit index to preserve monotonicity while issuing a local query to a Raft node. If the local commit index of a Raft node is smaller than the commit index passed to the RaftNode.query() call, the returned CompletableFuture object fails with LaggingCommitIndexException . This exception means that the state observed by the client is more up-to-date than the contacted Raft node's state. In this case, the client can retry its query on another Raft node. Please refer to \u00a7 6.4.1 of the Raft dissertation for more details. We will make a little trick to demonstrate how to maintain the monotonicity of the observed Raft group state for the local query policies . Recall that LocalTransport keeps a discovery map to send Raft messages to target Raft nodes. When we create a new Raft node, we add it to the other Raft nodes' discovery maps . This time, we will do exactly the reverse to block the communication between the leader and the follower. Let's add the following method to our LocalTransport class: public void undiscoverNode(RaftNode node) { RaftEndpoint endpoint = node.getLocalEndpoint(); if (localEndpoint.equals(endpoint)) { throw new IllegalArgumentException(localEndpoint + \" cannot undiscover itself!\"); } nodes.remove(node.getLocalEndpoint(), node); } We use this method in our new test below. We block the communication between the leader and follower after we set a value to the atomic register. Then we set another value which will not be replicated to the disconnected follower. After this step, we issue a query on the leader. We are also tracking the commit index we observed. Now, we switch to the disconnected follower and issue a new local query by passing the last observed commit index. Since the disconnected follower does not have the second commit, it cannot satisfy the monotonicity we demand, hence our query fails with LaggingCommitIndexException . To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.MonotonicLocalQueryTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository .","title":"Monotonic local queries"},{"location":"docs/tutorial-building-an-atomic-register/#4-snapshotting","text":"Now it is time to implement the missing snapshotting functionality in our atomic register state machine. Let's talk about snapshotting first. The Raft log grows during the lifecycle of the Raft group as more operations are committed. However, in a real-world system we cannot allow it to grow unboundedly. As the Raft log grows longer, it will consume more space both in memory and disk, and cause a lagging follower to catch up with the majority in a longer duration. Raft solves this problem by taking a snapshot of the state machine at current commit index and discarding all log entries up to it. MicroRaft implements snapshotting by putting an upper bound on the number of log entries kept in Raft log. It takes a snapshot of the state machine at every N commits and shrinks the log. N is configurable via RaftConfig.setCommitCountToTakeSnapshot() . A snapshot is represented as a list of chunks, where a chunk can be any object provided by the state machine. StateMachine contains 2 methods for the snapshotting functionality: takeSnapshot() and installSnapshot() . Similar to what we did in the previous part, we will extend OperableAtomicRegister to implement these methods. Since our atomic register state machine consists of a single value, we just create a single snapshot chunk object in takeSnapshot() . Dually, to install a snapshot, we overwrite the value of the atomic register with the value present in the received snapshot chunk object. MicroRaft guarantees that commit index of an installed snapshot is always greater than the last commit index observed by the state machine. You can also see this class in the MicroRaft Github repository . We have the following test to demonstrate how snapshotting works in MicroRaft. In createRaftNode() , we configure our Raft nodes to take a new snapshot at every 100 commits. Similar to what we did in the previous test, we block the communication between the leader and a follower, and fill up the leader's Raft log with new commits until it takes a snapshot. When we allow the leader to communicate with the follower again, the follower catches up with the leader by transferring the snapshot. To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.SnapshotInstallationTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository .","title":"4. Snapshotting"},{"location":"docs/tutorial-building-an-atomic-register/#5-extending-the-raft-group","text":"Ok. We covered a lot of topics up until this part. There are only a few things left to discuss. In this part, we will see how we can perform changes in Raft group member lists.","title":"5. Extending the Raft group"},{"location":"docs/tutorial-building-an-atomic-register/#changing-the-raft-group-member-list","text":"MicroRaft supports membership changes in Raft groups via RaftNode.changeMembership() . Let's first see the rules to realize membership changes in Raft groups. Raft group membership changes are appended to the internal Raft log as regular log entries and committed similar to user-supplied operations. Therefore, Raft group membership changes require the majority of the Raft group to be operational. When a membership change is committed, its commit index is used to denote the new member list of the Raft group and called group members commit index . Relatedly, when a membership change is triggered via RaftNode.changeMembership() , the current group members commit index must be provided. Last, MicroRaft allows one membership change at a time in a Raft group and more complex changes must be applied as a series of single changes. Since we know the rules for member list changes now, let's see some code. In our last test, we want to improve our 3-member Raft group's degree of fault tolerance by adding 2 new members (majority quorum size of 3 = 2 -> majority quorum size of 5 = 3). To run this test on your machine, try the following: $ gh repo clone MicroRaft/MicroRaft $ cd MicroRaft && ./mvnw clean test -Dtest=io.microraft.tutorial.ChangeRaftGroupMemberListTest -DfailIfNoTests=false -Ptutorial You can also see it in the MicroRaft Github repository . In this test, we create a new Raft endpoint, endpoint4 , and add it to the Raft group in the following lines: RaftEndpoint endpoint4 = LocalRaftEndpoint.newEndpoint(); // group members commit index of the initial Raft group members is 0. RaftGroupMembers newMemberList1 = leader.changeMembership(endpoint4, MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER, 0).join().getResult(); System.out.println(\"New member list: \" + newMemberList1.getMembers() + \", majority: \" + newMemberList1.getMajorityQuorumSize() + \", commit index: \" + newMemberList1.getLogIndex()); // endpoint4 is now part of the member list. Let's start its Raft node RaftNode raftNode4 = createRaftNode(endpoint4); raftNode4.start(); Please notice that we pass 0 for the third argument to RaftNode.changeMembership() , because our Raft group operates with the initial member list whose group members commit index is 0 . After this call returns, endpoint4 is part of the Raft group, so we start its Raft node as well. Our sysout line here prints the following: New member list: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}, LocalRaftEndpoint{id=node4}], majority: 3, commit index: 3 As you see, our Raft group has 4 members now, whose majority quorum size is 3. Actually, running a 4-node Raft group has no advantage over running a 3-node Raft group in terms of the degree of availability because both of them can handle failure of only 1 Raft node. Since we want to achieve better availability, we will add one more Raft node to our Raft group. So we create another Raft endpoint, endpoint5 , add it to the Raft group, and start its Raft node. However, as the group members commit index parameter, we pass newMemberList1.getLogIndex() which is the log index endpoint4 is added to the Raft group. Please note that we could also get the current Raft group members commit index via RaftNode.getCommittedMembers() . Our second sysout line prints the following: New member list: [LocalRaftEndpoint{id=node1}, LocalRaftEndpoint{id=node2}, LocalRaftEndpoint{id=node3}, LocalRaftEndpoint{id=node4}, LocalRaftEndpoint{id=node5}], majority: 3, commit index: 5 Now we have 5 nodes in our Raft group with the majority quorum size of 3. It means that now our Raft group can tolerate failure of 2 Raft nodes and still remain operational. Voila! In this example, for the sake of simplicity, we add new Raft nodes to the Raft group with MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER . With this mode, the Raft node is directly added as a follower , and the majority quorum size of the Raft group increases to 3. However, in real life use cases, Raft nodes can contain large state, and it may take some time until the new Raft node catches up with the leader. Because of this, increasing the majority quorum size may cause availability gaps if failures occur before the new Raft node catches up. To prevent this, MicroRaft offers another membership change mode, MembershipChangeMode.ADD_LEARNER , to add new Raft nodes. With this mode, the new Raft node is added with the learner role. Learner Raft nodes are excluded in majority calculations, hence adding a new learner Raft node to the Raft group does not change the majority quorum size. Once the new learner Raft node catches up, it can be promoted to the follower role by triggering another membership change: MembershipChangeMode.ADD_OR_PROMOTE_TO_FOLLOWER .","title":"Changing the Raft group member list"},{"location":"docs/tutorial-building-an-atomic-register/#whats-next","text":"In the next section, we will see how MicroRaft deals with failures. Just keep calm and carry on !","title":"What's next?"}]}